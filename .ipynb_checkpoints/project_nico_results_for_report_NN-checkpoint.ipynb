{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations_clean import *\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X,ids = load_csv_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_means = np.nanmean(X, axis=0)\n",
    "X = np.where(X == -999., np.nan, X)\n",
    "col_means = np.nanmedian(X, axis=0)\n",
    "idxs = np.where(np.isnan(X))\n",
    "X[idxs] = np.take(col_means, idxs[1])\n",
    "\n",
    "#feature 1: correlations der_mass_MMC\n",
    "X_gt_mmc = np.array(X[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X[:,0][X[:,0] > 140] = 140\n",
    "X = np.column_stack((X, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X[:,13]*np.cos(X[:,15])\n",
    "tau_py = X[:,13]*np.sin(X[:,15])\n",
    "tau_pz = X[:,13]*np.sinh(X[:,14])\n",
    "X = np.column_stack((X, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X[:,16]*np.cos(X[:,18])\n",
    "lep_py = X[:,16]*np.cos(X[:,18])\n",
    "lep_pz = X[:,16]*np.cos(X[:,17])\n",
    "X = np.column_stack((X, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X[:,23]*np.cos(X[:,25])\n",
    "jet_py = X[:,23]*np.cos(X[:,25])\n",
    "jet_pz = X[:,23]*np.cos(X[:,24])\n",
    "X = np.column_stack((X, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X[:,26]*np.cos(X[:,28])\n",
    "subjet_py = X[:,26]*np.cos(X[:,28])\n",
    "subjet_pz = X[:,26]*np.cos(X[:,27])\n",
    "X = np.column_stack((X, subjet_px,subjet_py,subjet_pz))\n",
    "#subleading jet momentum\n",
    "# DER_met_phi_centrality_cos = np.cos(X[:,11])\n",
    "# DER_met_phi_centrality_sin = np.sin(X[:,11])\n",
    "# X = np.column_stack((X, DER_met_phi_centrality_cos,DER_met_phi_centrality_sin))\n",
    "\n",
    "#feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X[:,11] = np.abs(X[:,11])\n",
    "#tau phi\n",
    "X[:,15] = np.abs(X[:,15])\n",
    "#lep phi\n",
    "X[:,18] = np.abs(X[:,18])\n",
    "#met phi\n",
    "X[:,20] = np.abs(X[:,20])\n",
    "#lead jet phi\n",
    "X[:,24] = np.abs(X[:,24])\n",
    "#sublead jet phi\n",
    "X[:,27] = np.abs(X[:,27])\n",
    "\n",
    "\n",
    "#feature 4: categorical PRI_jet_num\n",
    "jet_num_0 = (X[:,22] == 0).astype(int)\n",
    "jet_num_1 = (X[:,22] == 1).astype(int)\n",
    "jet_num_2 = (X[:,22] == 2).astype(int)\n",
    "jet_num_3 = (X[:,22] == 3).astype(int)\n",
    "\n",
    "# #feature 5: pt ratios\n",
    "# #tau_lep_ratio = PRI_tau_pt/PRI_lep_pt\n",
    "# tau_lep_ratio = X[:,13]/X[:,16]\n",
    "tau_lep_ratio = X[:,13]/X[:,16]\n",
    "# #jets_ratio = PRI_jet_leading_pt/PRI_jet_subleading_pt\n",
    "# jets_ratio = X[:,22]/X[:,25]\n",
    "# jets_ratio = X[:,23]/X[:,25]\n",
    "# #met_tot_ratio = PRI_met/PRI_met_sumet\n",
    "met_tot_ratio = X[:,19]/X[:,21]\n",
    "# X = np.column_stack((X, tau_lep_ratio,jets_ratio,met_tot_ratio))\n",
    "X = np.column_stack((X, tau_lep_ratio,met_tot_ratio))\n",
    "\n",
    "# #feature 6: jets_diff_angle\n",
    "jets_diff_angle = np.cos(X[:,24]-X[:,27])\n",
    "X = np.column_stack((X, jets_diff_angle))\n",
    "\n",
    "#TEST EXTRA COS/SIN ANGLES\n",
    "\n",
    "# df = pd.DataFrame(X)\n",
    "# df.head()\n",
    "# print(X[:,22] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    # converting -999. to nan to use np.nanmean and np.nanstd\n",
    "    X = np.where(X == -999., np.nan, X)\n",
    "    # standardizing the data Xd = (X_d - E[X_d])/(std(X_d))\n",
    "    X, means, stds = standardize(X)\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_features(X)\n",
    "X = np.column_stack((X, jet_num_0, jet_num_1, jet_num_2, jet_num_3))\n",
    "# X = np.delete(X,22,1) #deleting jet num reduces performance slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 51)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(X)\n",
    "cutoff = int(0.8*((X.shape)[0]))\n",
    "X_train = X[:cutoff]\n",
    "y_train = y[:cutoff]\n",
    "X_test = X[cutoff:]\n",
    "y_test = y[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:    \n",
    "    #activations: 'relu', 'sigmoid', 'linear'\n",
    "    #loss assumed to be BCE\n",
    "    def __init__(self, gamma = 0.001,  dimensions = [2,10,1], activations = ['relu','sigmoid'] ,weight_decay = 0):\n",
    "        assert (len(dimensions)-1) == len(activations), \"Number of dimensions and activation functions do not match\"\n",
    "        # number of layers of our MLP\n",
    "        self.num_layers = len(dimensions)\n",
    "        self.gamma = gamma\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        # the first layer is the input data\n",
    "        self.activations = {}\n",
    "        self.activations_grad = {}\n",
    "        \n",
    "        for n in np.arange(self.num_layers - 1):\n",
    "            # the weights are initialized acccording to a normal distribution and divided by the size of the layer they're on\n",
    "            self.weights[n + 1] = np.random.randn(dimensions[n + 1],dimensions[n]) / np.sqrt(dimensions[n])\n",
    "            # bias are all initialized to zero\n",
    "            self.bias[n + 1] = np.zeros(dimensions[n + 1])\n",
    "            \n",
    "            if activations[n] == 'relu':\n",
    "                self.activations[n+1] = self.relu\n",
    "                self.activations_grad[n+1] = self.relu_gradient\n",
    "            elif activations[n] == 'sigmoid':\n",
    "                self.activations[n+1] = self.sigmoid\n",
    "                self.activations_grad[n+1] = self.sigmoid_gradient\n",
    "            else:\n",
    "                self.activations[n+1] = lambda x : x\n",
    "                self.activations_grad[n+1] = lambda x : 1\n",
    "    \n",
    "    def feed_forward(self, x):        \n",
    "        # keep track of all z and a to compute gradient in the backpropagation\n",
    "        z = {}\n",
    "        # the first layer is the input data\n",
    "        a = {1:x}\n",
    "        # We compute z[n+1] = a[n] * w[n] + b[n]\n",
    "        # and a[n+1] = f(z[n+1]) = f(a[n] * x[n] + b[n]) where * is the inner product\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            z[n + 1] = self.weights[n] @ a[n] + self.bias[n]\n",
    "            a[n + 1] = self.activations[n](z[n + 1])\n",
    "        y_pred = a[n+1]    \n",
    "        return y_pred,a, z\n",
    "    \n",
    "    # returns a prediction\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y_i_proba,_,_ = self.feed_forward(X[i].squeeze()) \n",
    "            preds[i] = (y_i_proba > 0.5)\n",
    "        return preds\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y_i_proba,_,_ = self.feed_forward(X[i].squeeze()) \n",
    "            preds[i] = y_i_proba\n",
    "        return preds\n",
    "    \n",
    "    def back_propagate(self, y,y_pred, a, z):\n",
    "        \n",
    "        weights_gradient = {}\n",
    "        bias_gradient = {}\n",
    "        \n",
    "        nabla = self.BCE_gradient(y,y_pred)\n",
    "        \n",
    "        for n in np.flip(np.arange(1, self.num_layers)):\n",
    "            nabla = nabla * self.activations_grad[n](z[n+1])\n",
    "            weights_gradient[n] = np.outer(nabla, a[n])\n",
    "            bias_gradient[n] = nabla\n",
    "            nabla = nabla @ self.weights[n]\n",
    "        \n",
    "        return weights_gradient, bias_gradient\n",
    "        ## self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "    \n",
    "    #weight decay : l2 reg\n",
    "    def gradient_descent_step(self, weights_gradient, bias_gradient):\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            self.weights[n] = self.weights[n] - self.gamma * (weights_gradient[n] + self.weight_decay*self.weights[n])\n",
    "            self.bias[n] = self.bias[n] - self.gamma * (bias_gradient[n] + self.weight_decay*self.bias[n])            \n",
    "    \n",
    "    #batch size = 1 for now\n",
    "    def train(self, X, y, max_iter, batch_size = 1, decay = False, decay_rate = 3, decay_iteration = 0):\n",
    "        for i in range(max_iter):\n",
    "            if (decay):\n",
    "                if ((i % decay_iteration == 0) and (i != 0)):\n",
    "                    print(\"Iteration: {}\".format(i))\n",
    "                    print(\"Decay, lr : {}\".format(self.gamma))\n",
    "                    self.gamma = self.gamma/decay_rate\n",
    "                    print(\"Decay, lr : {}\".format(self.gamma))\n",
    "                    print(\"\")\n",
    "            idxs = np.random.randint(0, X.shape[0],batch_size)\n",
    "            X_batch = X[idxs].squeeze()\n",
    "            y_batch = y[idxs]\n",
    "            y_pred,a, z = self.feed_forward(X_batch)\n",
    "            weights_gradient, bias_gradient = self.back_propagate(y_batch,y_pred,a, z)\n",
    "            self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "            if ((i % int(max_iter/10)) == 0):\n",
    "                loss = self.BCE_loss(X,y)\n",
    "                print(\"Iteration : {}, loss : {}\".format(i,loss))\n",
    "        loss = self.BCE_loss(X,y)\n",
    "        print(\"Iteration : {}, loss : {}\".format(i,loss))\n",
    "        return loss\n",
    "            \n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_gradient(self,z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "    \n",
    "    def relu(self,z):\n",
    "        return np.where(z < 0, 0, z)\n",
    "\n",
    "    def relu_gradient(self, z):\n",
    "        return np.where(z < 0, 0, 1)\n",
    "        \n",
    "    #check if possible to vectorize\n",
    "    def BCE_loss(self,X, y):\n",
    "        loss = 0\n",
    "        N = len(y)\n",
    "        for i in range(N):\n",
    "            y_pred,_,_ = self.feed_forward(X[i])\n",
    "            eps = 1e-7\n",
    "            loss_i = -(y[i]*np.log(y_pred+eps) + (1-y[i])*np.log(1-y_pred+eps))\n",
    "            loss = loss + loss_i/N\n",
    "        return loss\n",
    "    \n",
    "    def BCE_gradient(self,y,y_pred):\n",
    "        #return y_pred-y\n",
    "        eps = 1e-7\n",
    "        return (-y/(y_pred+eps) + (1-y)/(1-y_pred+eps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, loss : [0.75308192]\n",
      "Iteration : 500000, loss : [0.37168098]\n",
      "Iteration : 1000000, loss : [0.36450644]\n",
      "Iteration : 1500000, loss : [0.36638023]\n",
      "Iteration: 2000000\n",
      "Decay, lr : 0.01\n",
      "Decay, lr : 0.002\n",
      "\n",
      "Iteration : 2000000, loss : [0.36302725]\n",
      "Iteration : 2500000, loss : [0.35231619]\n",
      "Iteration : 3000000, loss : [0.35291384]\n",
      "Iteration : 3500000, loss : [0.35217554]\n",
      "Iteration: 4000000\n",
      "Decay, lr : 0.002\n",
      "Decay, lr : 0.0004\n",
      "\n",
      "Iteration : 4000000, loss : [0.35052864]\n",
      "Iteration : 4500000, loss : [0.34719996]\n",
      "Iteration : 4999999, loss : [0.3468192]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.3468192])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "in_dim = X_train.shape[1]\n",
    "n_h1 = 30\n",
    "n_h2 = 30\n",
    "n_h3 = 30\n",
    "# n_h4 = 30\n",
    "out_dim = 1\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,out_dim]\n",
    "activations = ['relu','relu','relu','sigmoid']\n",
    "gamma = 0.01\n",
    "weight_decay = 0.0001\n",
    "mlp = MLP(gamma = gamma, dimensions = dimensions, activations = activations,\n",
    "          weight_decay = weight_decay)\n",
    "mlp.train(X_train,y_train,max_iter = 5000000,decay_rate = 5,decay_iteration = 2000000,decay = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84638\n"
     ]
    }
   ],
   "source": [
    "#train accuracy\n",
    "y_pred = mlp.predict(X_train)\n",
    "acc = 1-np.sum(np.abs(y_pred - y_train)) / X_train.shape[0]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84194\n"
     ]
    }
   ],
   "source": [
    "#test accuracy\n",
    "y_pred = mlp.predict(X_test)\n",
    "acc = 1-np.sum(np.abs(y_pred - y_test)) / X_test.shape[0]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,X_sub,ids = load_csv_data(\"test.csv\")\n",
    "#feature 1: correlations der_mass_MMC\n",
    "X_sub = np.where(X_sub == -999., np.nan, X_sub)\n",
    "# col_means = np.nanmean(X_sub, axis=0)\n",
    "col_means = np.nanmedian(X_sub, axis=0)\n",
    "idxs = np.where(np.isnan(X_sub))\n",
    "X_sub[idxs] = np.take(col_means, idxs[1])\n",
    "X_gt_mmc = np.array(X_sub[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X_sub[:,0][X_sub[:,0] > 140] = 140\n",
    "X_sub = np.column_stack((X_sub, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X_sub[:,13]*np.cos(X_sub[:,15])\n",
    "tau_py = X_sub[:,13]*np.sin(X_sub[:,15])\n",
    "tau_pz = X_sub[:,13]*np.sinh(X_sub[:,14])\n",
    "X_sub = np.column_stack((X_sub, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X_sub[:,16]*np.cos(X_sub[:,18])\n",
    "lep_py = X_sub[:,16]*np.cos(X_sub[:,18])\n",
    "lep_pz = X_sub[:,16]*np.cos(X_sub[:,17])\n",
    "X_sub = np.column_stack((X_sub, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X_sub[:,22]*np.cos(X_sub[:,24])\n",
    "jet_py = X_sub[:,22]*np.cos(X_sub[:,24])\n",
    "jet_pz = X_sub[:,22]*np.cos(X_sub[:,23])\n",
    "X_sub = np.column_stack((X_sub, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X_sub[:,25]*np.cos(X_sub[:,27])\n",
    "subjet_py = X_sub[:,25]*np.cos(X_sub[:,27])\n",
    "subjet_pz = X_sub[:,25]*np.cos(X_sub[:,26])\n",
    "X_sub = np.column_stack((X_sub, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "# feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X_sub[:,11] = np.abs(X_sub[:,11])\n",
    "#tau phi\n",
    "X_sub[:,15] = np.abs(X_sub[:,15])\n",
    "#lep phi\n",
    "X_sub[:,18] = np.abs(X_sub[:,18])\n",
    "#met phi\n",
    "X_sub[:,20] = np.abs(X_sub[:,20])\n",
    "#lead jet phi\n",
    "X_sub[:,24] = np.abs(X_sub[:,24])\n",
    "#sublead jet phi\n",
    "X_sub[:,27] = np.abs(X_sub[:,27])\n",
    "\n",
    "#feature 4: categorical PRI_jet_num\n",
    "jet_num_0 = (X_sub[:,22] == 0).astype(int)\n",
    "jet_num_1 = (X_sub[:,22] == 1).astype(int)\n",
    "jet_num_2 = (X_sub[:,22] == 2).astype(int)\n",
    "jet_num_3 = (X_sub[:,22] == 3).astype(int)\n",
    "\n",
    "#feature 5: pt ratios\n",
    "# tau_lep_ratio = PRI_tau_pt/PRI_lep_pt\n",
    "tau_lep_ratio = X_sub[:,13]/X_sub[:,16]\n",
    "#met_tot_ratio = PRI_met/PRI_met_sumet\n",
    "met_tot_ratio = X_sub[:,19]/X_sub[:,21]\n",
    "X_sub = np.column_stack((X_sub, tau_lep_ratio,met_tot_ratio))\n",
    "\n",
    "# #feature 6: jets_diff_angle\n",
    "jets_diff_angle = np.cos(X_sub[:,24]-X_sub[:,27])\n",
    "X_sub = np.column_stack((X_sub, jets_diff_angle))\n",
    "\n",
    "X_sub = make_features(X_sub)\n",
    "X_sub = np.column_stack((X_sub, jet_num_0, jet_num_1, jet_num_2, jet_num_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = mlp.predict(X_sub)\n",
    "#map to -1,1\n",
    "sub_pred = sub_pred*2 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3815197153305481"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, sub_pred, \"submission_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
