{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations_clean import *\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X,ids = load_csv_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.where(X == -999., np.nan, X)\n",
    "# y_1 = y[~np.isnan(X).any(axis=1)]\n",
    "# X_1 = X[~np.isnan(X).any(axis=1)]\n",
    "# y_2 = y[np.isnan(X).any(axis=1)]\n",
    "# X_2 = X[np.isnan(X).any(axis=1)]\n",
    "# cnd_1 = ~np.isnan(X).any(axis=1)\n",
    "# cnd_2 = np.isnan(X[:,23])\n",
    "# cnd_3 = (~cnd_1) & (~cnd_2)\n",
    "# cnd_1 = ~np.isnan(X).any(axis=1)\n",
    "# cnd_2 = np.isnan(X[:,23])\n",
    "# cnd_3 = (~cnd_1) & (~cnd_2)\n",
    "\n",
    "\n",
    "\n",
    "# cnd_1 = X[:,22] <= 1\n",
    "# cnd_2 = X[:,22] >= 2\n",
    "\n",
    "# cnd_1 = X[:,22] == 0\n",
    "# cnd_2 = X[:,22] == 1\n",
    "# cnd_3 = X[:,22] >= 2\n",
    "\n",
    "cnd_1 = X[:,22] == 0\n",
    "cnd_2 = X[:,22] == 1\n",
    "cnd_3 = X[:,22] == 2\n",
    "cnd_4 = X[:,22] == 3\n",
    "\n",
    "y_1 = y[cnd_1]\n",
    "X_1 = X[cnd_1]\n",
    "y_2 = y[cnd_2]\n",
    "X_2 = X[cnd_2]\n",
    "y_3 = y[cnd_3]\n",
    "X_3 = X[cnd_3]\n",
    "y_4 = y[cnd_4]\n",
    "X_4 = X[cnd_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 30)\n",
      "(77544, 30)\n",
      "(50379, 30)\n",
      "(22164, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_1.shape)\n",
    "print(X_2.shape)\n",
    "print(X_3.shape)\n",
    "print(X_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    # converting -999. to nan to use np.nanmean and np.nanstd\n",
    "    X = np.where(X == -999., np.nan, X)\n",
    "    # standardizing the data Xd = (X_d - E[X_d])/(std(X_d))\n",
    "    X, means, stds = standardize(X)\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_2(X):\n",
    "    for i in range(X.shape[1]):\n",
    "        if (X[:,i].std() == 0):\n",
    "            X[:,i] = 0\n",
    "        else:\n",
    "            X[:,i] = (X[:,i] - X[:,i].mean()) / X[:,i].std()\n",
    "#     X, means, stds = standardize(X)\n",
    "#     X = (X-X.nanmean())/X.nanstd()\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(X):\n",
    "    for i in range(X.shape[1]):\n",
    "        if (np.isnan(X[:,i]).all()):\n",
    "            X[:,i] = 0\n",
    "        else:\n",
    "            col_means = np.nanmedian(X[:,i])\n",
    "            idxs = np.where(np.isnan(X[:,i]))\n",
    "            X[idxs,i] = col_means\n",
    "\n",
    "#     feature 1: correlations der_mass_MMC\n",
    "    X_gt_mmc = np.array(X[:,0], copy=True)\n",
    "    # X_0_cop = np.array(X[:,0], copy=True)\n",
    "    X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "    # X = np.column_stack((X, X_gt_mmc))\n",
    "    X[:,0][X[:,0] > 140] = 140\n",
    "    X = np.column_stack((X, X_gt_mmc))\n",
    "\n",
    "    #feature 2: add momentums\n",
    "    #tau momentum\n",
    "    tau_px = X[:,13]*np.cos(X[:,15])\n",
    "    tau_py = X[:,13]*np.sin(X[:,15])\n",
    "    tau_pz = X[:,13]*np.sinh(X[:,14])\n",
    "    tau_mod = X[:,13]*np.cosh(X[:,14])\n",
    "    X = np.column_stack((X, tau_px,tau_py,tau_pz,tau_mod))\n",
    "    #lep momentum\n",
    "    lep_px = X[:,16]*np.cos(X[:,18])\n",
    "    lep_py = X[:,16]*np.sin(X[:,18])\n",
    "    lep_pz = X[:,16]*np.sinh(X[:,17])\n",
    "    lep_mod = X[:,16]*np.cosh(X[:,17])\n",
    "    X = np.column_stack((X, lep_px,lep_py,lep_pz,lep_mod))\n",
    "    #leading jet momentum\n",
    "    jet_px = X[:,23]*np.cos(X[:,25])\n",
    "    jet_py = X[:,23]*np.sin(X[:,25])\n",
    "    jet_pz = X[:,23]*np.sinh(X[:,24])\n",
    "    jet_mod = X[:,23]*np.cosh(X[:,24])\n",
    "    X = np.column_stack((X, jet_px,jet_py,jet_pz,jet_mod))\n",
    "    #subleading jet momentum\n",
    "    subjet_px = X[:,26]*np.cos(X[:,28])\n",
    "    subjet_py = X[:,26]*np.sin(X[:,28])\n",
    "    subjet_pz = X[:,26]*np.sinh(X[:,27])\n",
    "    subjet_mod = X[:,26]*np.cosh(X[:,27])\n",
    "    X = np.column_stack((X, subjet_px,subjet_py,subjet_pz,subjet_mod))\n",
    "\n",
    "\n",
    "    #feature 8: total invariant mass\n",
    "    term_1 = np.sqrt(tau_px**2 + tau_py**2 + tau_pz**2) + np.sqrt(lep_px**2 + lep_py**2 + lep_pz**2) \\\n",
    "    + np.sqrt(jet_px**2 + jet_py**2 + jet_pz**2) + np.sqrt(subjet_px**2 + subjet_py**2 + subjet_pz**2)\n",
    "    term_2 = (tau_px + lep_px + jet_px + subjet_px)**2 + (tau_py + lep_py + jet_py + subjet_py)**2 \\\n",
    "            + (tau_pz + lep_pz + jet_pz + subjet_pz)**2\n",
    "    inv_mass = np.sqrt(term_1**2 - term_2)\n",
    "    X = np.column_stack((X, inv_mass))\n",
    "\n",
    "#     feature 9: inverse log\n",
    "    inv_log_cols = [0,1,2,3,4,5,7,8,9,10,12,13,16,19,21,23,26]\n",
    "    X_inv_log_cols = np.log(1 / (1 + X[:, inv_log_cols]))\n",
    "    X = np.hstack((X, X_inv_log_cols))\n",
    "\n",
    "\n",
    "    # #feature 5: pt ratios\n",
    "    # #tau_lep_ratio = PRI_tau_pt/PRI_lep_pt\n",
    "    tau_lep_ratio = X[:,13]/X[:,16]\n",
    "    # #met_tot_ratio = PRI_met/PRI_met_sumet\n",
    "    met_tot_ratio = X[:,19]/X[:,21]\n",
    "    X = np.column_stack((X, tau_lep_ratio,met_tot_ratio))\n",
    "\n",
    "    #X = make_features(X)\n",
    "    X = make_features_2(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = preproc(X)\n",
    "X_1 = preproc(X_1)\n",
    "X_2 = preproc(X_2)\n",
    "X_3 = preproc(X_3)\n",
    "X_4 = preproc(X_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.shape)\n",
    "# print(X_1.shape)\n",
    "# print(X_2.shape)\n",
    "# print(X_3.shape)\n",
    "# print(X_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_1 = int(0.8*((X_1.shape)[0]))\n",
    "cutoff_2 = int(0.8*((X_2.shape)[0]))\n",
    "cutoff_3 = int(0.8*((X_3.shape)[0]))\n",
    "cutoff_4 = int(0.8*((X_4.shape)[0]))\n",
    "X_1_train = X_1#[:cutoff_1]\n",
    "y_1_train = y_1#[:cutoff_1]\n",
    "X_1_test = X_1[cutoff_1:]\n",
    "y_1_test = y_1[cutoff_1:]\n",
    "X_2_train = X_2#[:cutoff_2]\n",
    "y_2_train = y_2#[:cutoff_2]\n",
    "X_2_test = X_2[cutoff_2:]\n",
    "y_2_test = y_2[cutoff_2:]\n",
    "X_3_train = X_3#[:cutoff_3]\n",
    "y_3_train = y_3#[:cutoff_3]\n",
    "X_3_test = X_3[cutoff_3:]\n",
    "y_3_test = y_3[cutoff_3:]\n",
    "X_4_train = X_4#[:cutoff_4]\n",
    "y_4_train = y_4#[:cutoff_4]\n",
    "X_4_test = X_4[cutoff_4:]\n",
    "y_4_test = y_4[cutoff_4:]\n",
    "\n",
    "# cutoff = int(0.8*((X.shape)[0]))\n",
    "# X_train = X[:cutoff]\n",
    "# y_train = y[:cutoff]\n",
    "# X_test = X[cutoff:]\n",
    "# y_test = y[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:    \n",
    "    #activations: 'relu', 'sigmoid', 'linear'\n",
    "    #loss assumed to be BCE\n",
    "    def __init__(self, gamma = 0.001,  dimensions = [2,10,1], activations = ['relu','sigmoid'] ,weight_decay = 0):\n",
    "        assert (len(dimensions)-1) == len(activations), \"Number of dimensions and activation functions do not match\"\n",
    "        # number of layers of our MLP\n",
    "        self.num_layers = len(dimensions)\n",
    "        self.gamma = gamma\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        # the first layer is the input data\n",
    "        self.activations = {}\n",
    "        self.activations_grad = {}\n",
    "        \n",
    "        for n in np.arange(self.num_layers - 1):\n",
    "            # the weights are initialized acccording to a normal distribution and divided by the size of the layer they're on\n",
    "            self.weights[n + 1] = np.random.randn(dimensions[n + 1],dimensions[n]) / np.sqrt(dimensions[n])\n",
    "            # bias are all initialized to zero\n",
    "            self.bias[n + 1] = np.zeros(dimensions[n + 1])\n",
    "            \n",
    "            if activations[n] == 'relu':\n",
    "                self.activations[n+1] = self.relu\n",
    "                self.activations_grad[n+1] = self.relu_gradient\n",
    "            elif activations[n] == 'sigmoid':\n",
    "                self.activations[n+1] = self.sigmoid\n",
    "                self.activations_grad[n+1] = self.sigmoid_gradient\n",
    "            else:\n",
    "                self.activations[n+1] = lambda x : x\n",
    "                self.activations_grad[n+1] = lambda x : 1\n",
    "    \n",
    "    def feed_forward(self, x):        \n",
    "        # keep track of all z and a to compute gradient in the backpropagation\n",
    "        z = {}\n",
    "        # the first layer is the input data\n",
    "        a = {1:x}\n",
    "        # We compute z[n+1] = a[n] * w[n] + b[n]\n",
    "        # and a[n+1] = f(z[n+1]) = f(a[n] * x[n] + b[n]) where * is the inner product\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            z[n + 1] = self.weights[n] @ a[n] + self.bias[n]\n",
    "            a[n + 1] = self.activations[n](z[n + 1])\n",
    "        y_pred = a[n+1]    \n",
    "        return y_pred,a, z\n",
    "    \n",
    "    # returns a prediction\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y_i_proba,_,_ = self.feed_forward(X[i].squeeze()) \n",
    "            preds[i] = (y_i_proba > 0.5)\n",
    "        return preds\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y_i_proba,_,_ = self.feed_forward(X[i].squeeze()) \n",
    "            preds[i] = y_i_proba\n",
    "        return preds\n",
    "    \n",
    "    def back_propagate(self, y,y_pred, a, z):\n",
    "        \n",
    "        weights_gradient = {}\n",
    "        bias_gradient = {}\n",
    "        \n",
    "        nabla = self.BCE_gradient(y,y_pred)\n",
    "        \n",
    "        for n in np.flip(np.arange(1, self.num_layers)):\n",
    "            nabla = nabla * self.activations_grad[n](z[n+1])\n",
    "            weights_gradient[n] = np.outer(nabla, a[n])\n",
    "            bias_gradient[n] = nabla\n",
    "            nabla = nabla @ self.weights[n]\n",
    "        \n",
    "        return weights_gradient, bias_gradient\n",
    "        ## self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "    \n",
    "    #weight decay : l2 reg\n",
    "    def gradient_descent_step(self, weights_gradient, bias_gradient):\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            self.weights[n] = self.weights[n] - self.gamma * (weights_gradient[n] + self.weight_decay*self.weights[n])\n",
    "            self.bias[n] = self.bias[n] - self.gamma * (bias_gradient[n] + self.weight_decay*self.bias[n])            \n",
    "    \n",
    "    #batch size = 1 for now\n",
    "    def train(self, X, y, max_iter, batch_size = 1, decay = False, decay_rate = 3, decay_iteration = 0):\n",
    "        for i in range(max_iter):\n",
    "            if (decay):\n",
    "                if ((i % decay_iteration == 0) and (i != 0)):\n",
    "                    print(\"Iteration: {}\".format(i))\n",
    "                    print(\"Decay, lr : {}\".format(self.gamma))\n",
    "                    self.gamma = self.gamma/decay_rate\n",
    "                    print(\"Decay, lr : {}\".format(self.gamma))\n",
    "                    print(\"\")\n",
    "            idxs = np.random.randint(0, X.shape[0],batch_size)\n",
    "            X_batch = X[idxs].squeeze()\n",
    "            y_batch = y[idxs]\n",
    "            y_pred,a, z = self.feed_forward(X_batch)\n",
    "            weights_gradient, bias_gradient = self.back_propagate(y_batch,y_pred,a, z)\n",
    "            self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "            if ((i % int(max_iter/5)) == 0):\n",
    "                loss = self.BCE_loss(X,y)\n",
    "                print(\"Iteration : {}, loss : {}\".format(i,loss))\n",
    "        loss = self.BCE_loss(X,y)\n",
    "        print(\"Iteration : {}, loss : {}\".format(i,loss))\n",
    "        return loss\n",
    "            \n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_gradient(self,z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "    \n",
    "    def relu(self,z):\n",
    "        return np.where(z < 0, 0, z)\n",
    "\n",
    "    def relu_gradient(self, z):\n",
    "        return np.where(z < 0, 0, 1)\n",
    "        \n",
    "    #check if possible to vectorize\n",
    "    def BCE_loss(self,X, y):\n",
    "        loss = 0\n",
    "        N = len(y)\n",
    "        eps = 1e-7\n",
    "        for i in range(N):\n",
    "            y_pred,_,_ = self.feed_forward(X[i])\n",
    "            loss_i = -(y[i]*np.log(y_pred+eps) + (1-y[i])*np.log(1-y_pred+eps))\n",
    "            loss = loss + loss_i/N\n",
    "        return loss\n",
    "    \n",
    "    def BCE_gradient(self,y,y_pred):\n",
    "        #return y_pred-y\n",
    "        eps = 1e-7\n",
    "        return (-y/(y_pred+eps) + (1-y)/(1-y_pred+eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, loss : [0.69633722]\n",
      "Iteration : 700000, loss : [0.34355258]\n",
      "Iteration : 1400000, loss : [0.35193106]\n",
      "Iteration: 1500000\n",
      "Decay, lr : 0.01\n",
      "Decay, lr : 0.002\n",
      "\n",
      "Iteration : 2100000, loss : [0.33568011]\n",
      "Iteration : 2800000, loss : [0.33482248]\n",
      "Iteration: 3000000\n",
      "Decay, lr : 0.002\n",
      "Decay, lr : 0.0004\n",
      "\n",
      "Iteration : 3499999, loss : [0.33010353]\n",
      "Iteration : 0, loss : [0.74811983]\n",
      "Iteration : 700000, loss : [0.40068914]\n",
      "Iteration : 1400000, loss : [0.39694094]\n",
      "Iteration: 1500000\n",
      "Decay, lr : 0.001\n",
      "Decay, lr : 0.0001\n",
      "\n",
      "Iteration : 2100000, loss : [0.38417023]\n",
      "Iteration : 2800000, loss : [0.38258103]\n",
      "Iteration: 3000000\n",
      "Decay, lr : 0.0001\n",
      "Decay, lr : 1e-05\n",
      "\n",
      "Iteration : 3499999, loss : [0.38110938]\n",
      "Iteration : 0, loss : [0.69937049]\n",
      "Iteration : 700000, loss : [0.36979772]\n",
      "Iteration : 1400000, loss : [0.3640491]\n",
      "Iteration: 1500000\n",
      "Decay, lr : 0.01\n",
      "Decay, lr : 0.002\n",
      "\n",
      "Iteration : 2100000, loss : [0.351313]\n",
      "Iteration : 2800000, loss : [0.33674966]\n",
      "Iteration: 3000000\n",
      "Decay, lr : 0.002\n",
      "Decay, lr : 0.0004\n",
      "\n",
      "Iteration : 3499999, loss : [0.31749893]\n",
      "Iteration : 0, loss : [0.71206942]\n",
      "Iteration : 700000, loss : [0.37414417]\n",
      "Iteration : 1400000, loss : [0.38871489]\n",
      "Iteration: 1500000\n",
      "Decay, lr : 0.01\n",
      "Decay, lr : 0.002\n",
      "\n",
      "Iteration : 2100000, loss : [0.35119415]\n",
      "Iteration : 2800000, loss : [0.3487817]\n",
      "Iteration: 3000000\n",
      "Decay, lr : 0.002\n",
      "Decay, lr : 0.0004\n",
      "\n",
      "Iteration : 3499999, loss : [0.33266228]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.33266228])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "in_dim = X_1.shape[1]\n",
    "n_h1 = 100\n",
    "n_h2 = 100\n",
    "n_h3 = 100\n",
    "n_h4 = 100\n",
    "n_h5 = 100\n",
    "n_h6 = 100\n",
    "n_h7 = 100\n",
    "out_dim = 1\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,n_h4,n_h5,n_h6,n_h7,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','relu','relu','sigmoid']\n",
    "gamma = 0.001\n",
    "weight_decay = 0.001\n",
    "# mlp_1 = MLP(gamma = gamma, dimensions = dimensions, activations = activations,\n",
    "#           weight_decay = 0.01)\n",
    "# mlp_2 = MLP(gamma = gamma, dimensions = dimensions, activations = activations,\n",
    "#           weight_decay = 0.01)\n",
    "# # mlp_2 = MLP(gamma = 0.001, dimensions = dimensions, activations = activations,\n",
    "# #           weight_decay = 0.003)\n",
    "# mlp_3 = MLP(gamma = gamma, dimensions = dimensions, activations = activations,\n",
    "#           weight_decay = 0.01)\n",
    "# mlp_4 = MLP(gamma = gamma, dimensions = dimensions, activations = activations,\n",
    "#           weight_decay = 0.01)\n",
    "# mlp_1.train(X_1_train,y_1_train,max_iter = 3500000,decay_rate = 10,decay_iteration = 1500000,decay = True)\n",
    "# mlp_2.train(X_2_train,y_2_train,max_iter = 8000000,decay_rate = 10,decay_iteration = 1500000,decay = True)\n",
    "# mlp_3.train(X_3_train,y_3_train,max_iter = 3500000,decay_rate = 10,decay_iteration = 1500000,decay = True)\n",
    "# mlp_4.train(X_4_train,y_4_train,max_iter = 3500000,decay_rate = 10,decay_iteration = 1500000,decay = True)\n",
    "\n",
    "mlp_1 = MLP(gamma = 0.01, dimensions = [in_dim,60,60,30,out_dim], activations = ['relu','relu','relu','sigmoid'],\n",
    "          weight_decay = 0.001)\n",
    "mlp_2 = MLP(gamma = 0.001, dimensions = [in_dim,60,60,60,30,out_dim], activations = ['relu','relu','relu','relu','sigmoid'],\n",
    "          weight_decay = 0.003)\n",
    "# mlp_2 = MLP(gamma = 0.001, dimensions = dimensions, activations = activations,\n",
    "#           weight_decay = 0.003)\n",
    "mlp_3 = MLP(gamma = 0.01, dimensions = [in_dim,60,60,30,out_dim], activations = ['relu','relu','relu','sigmoid'],\n",
    "          weight_decay = 0.003)\n",
    "mlp_4 = MLP(gamma = 0.01, dimensions = [in_dim,60,30,out_dim], activations = ['relu','relu','sigmoid'],\n",
    "          weight_decay = 0.005)\n",
    "mlp_1.train(X_1_train,y_1_train,max_iter = 3500000,decay_rate = 5,decay_iteration = 1500000,decay = True)\n",
    "mlp_2.train(X_2_train,y_2_train,max_iter = 3500000,decay_rate = 10,decay_iteration = 1500000,decay = True)\n",
    "mlp_3.train(X_3_train,y_3_train,max_iter = 3500000,decay_rate = 5,decay_iteration = 1500000,decay = True)\n",
    "mlp_4.train(X_4_train,y_4_train,max_iter = 3500000,decay_rate = 5,decay_iteration = 1500000,decay = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8537027213675898\n",
      "0.8282394511503146\n",
      "0.8624228349113718\n",
      "0.8589604764482945\n"
     ]
    }
   ],
   "source": [
    "#train accuracy\n",
    "# y_pred = mlp.predict(X_train)\n",
    "# acc = 1-np.sum(np.abs(y_pred - y_train)) / X_train.shape[0]\n",
    "# print(acc)\n",
    "y_1_pred = mlp_1.predict(X_1_train)\n",
    "acc1 = 1-np.sum(np.abs(y_1_pred - y_1_train)) / X_1_train.shape[0]\n",
    "print(acc1)\n",
    "y_2_pred = mlp_2.predict(X_2_train)\n",
    "acc2 = 1-np.sum(np.abs(y_2_pred - y_2_train)) / X_2_train.shape[0]\n",
    "print(acc2)\n",
    "y_3_pred = mlp_3.predict(X_3_train)\n",
    "acc3 = 1-np.sum(np.abs(y_3_pred - y_3_train)) / X_3_train.shape[0]\n",
    "print(acc3)\n",
    "y_4_pred = mlp_4.predict(X_4_train)\n",
    "acc4 = 1-np.sum(np.abs(y_4_pred - y_4_train)) / X_4_train.shape[0]\n",
    "print(acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851523795225942\n",
      "0.8276484621832485\n",
      "0.8632393807066296\n",
      "0.8687119332280623\n"
     ]
    }
   ],
   "source": [
    "#test accuracy\n",
    "# y_pred = mlp.predict(X_test)\n",
    "# acc = 1-np.sum(np.abs(y_pred - y_test)) / X_test.shape[0]\n",
    "# print(acc)\n",
    "y_1_pred = mlp_1.predict(X_1_test)\n",
    "acc1 = 1-np.sum(np.abs(y_1_pred - y_1_test)) / X_1_test.shape[0]\n",
    "print(acc1)\n",
    "y_2_pred = mlp_2.predict(X_2_test)\n",
    "acc2 = 1-np.sum(np.abs(y_2_pred - y_2_test)) / X_2_test.shape[0]\n",
    "print(acc2)\n",
    "y_3_pred = mlp_3.predict(X_3_test)\n",
    "acc3 = 1-np.sum(np.abs(y_3_pred - y_3_test)) / X_3_test.shape[0]\n",
    "print(acc3)\n",
    "y_4_pred = mlp_4.predict(X_4_test)\n",
    "acc4 = 1-np.sum(np.abs(y_4_pred - y_4_test)) / X_4_test.shape[0]\n",
    "print(acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,X_sub,ids = load_csv_data(\"test.csv\")\n",
    "# X_sub = np.where(X_sub == -999., np.nan, X_sub)\n",
    "# no_nan_idxs = ~np.isnan(X_sub).any(axis=1)\n",
    "# nan_idxs = np.isnan(X_sub).any(axis=1)\n",
    "\n",
    "# X_sub_1 = X_sub[~np.isnan(X_sub).any(axis=1)]\n",
    "# X_sub_2 = X_sub[np.isnan(X_sub).any(axis=1)]\n",
    "\n",
    "# X_sub_1 = preproc(X_sub_1)\n",
    "# X_sub_2 = preproc(X_sub_2)\n",
    "\n",
    "# sub_1_pred = mlp_1.predict(X_sub_1)\n",
    "# sub_2_pred = mlp_2.predict(X_sub_2)\n",
    "# sub_1_pred = sub_1_pred*2 -1\n",
    "# sub_2_pred = sub_2_pred*2 -1\n",
    "# sub_pred = np.zeros(X_sub.shape[0])\n",
    "# sub_pred[no_nan_idxs] = sub_1_pred\n",
    "# sub_pred[nan_idxs] = sub_2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,X_sub,ids = load_csv_data(\"test.csv\")\n",
    "X_sub = np.where(X_sub == -999., np.nan, X_sub)\n",
    "\n",
    "# X_sub = preproc(X_sub)\n",
    "# sub_pred = mlp.predict(X_sub)\n",
    "# sub_pred = sub_pred*2 -1\n",
    "\n",
    "# cnd_1 = ~np.isnan(X_sub).any(axis=1)\n",
    "# cnd_2 = np.isnan(X_sub[:,23])\n",
    "# cnd_3 = (~cnd_1) & (~cnd_2)\n",
    "\n",
    "# X_sub_1 = X_sub[cnd_1]\n",
    "# X_sub_2 = X_sub[cnd_2]\n",
    "# X_sub_3 = X_sub[cnd_3]\n",
    "\n",
    "# X_sub_1 = preproc(X_sub_1)\n",
    "# X_sub_2 = preproc(X_sub_2)\n",
    "# X_sub_3 = preproc(X_sub_3)\n",
    "\n",
    "# sub_1_pred = mlp_1.predict(X_sub_1)\n",
    "# sub_2_pred = mlp_2.predict(X_sub_2)\n",
    "# sub_3_pred = mlp_3.predict(X_sub_3)\n",
    "# sub_1_pred = sub_1_pred*2 -1\n",
    "# sub_2_pred = sub_2_pred*2 -1\n",
    "# sub_3_pred = sub_3_pred*2 -1\n",
    "# sub_pred = np.zeros(X_sub.shape[0])\n",
    "# sub_pred[cnd_1] = sub_1_pred\n",
    "# sub_pred[cnd_2] = sub_2_pred\n",
    "# sub_pred[cnd_3] = sub_3_pred\n",
    "\n",
    "\n",
    "# cnd_1 = ~np.isnan(X_sub).any(axis=1)\n",
    "# cnd_2 = np.isnan(X_sub[:,23])\n",
    "# cnd_3 = (~cnd_1) & (~cnd_2)\n",
    "\n",
    "cnd_1 = X_sub[:,22] == 0\n",
    "cnd_2 = X_sub[:,22] == 1\n",
    "cnd_3 = X_sub[:,22] == 2\n",
    "cnd_4 = X_sub[:,22] == 3\n",
    "\n",
    "X_sub_1 = X_sub[cnd_1]\n",
    "X_sub_2 = X_sub[cnd_2]\n",
    "X_sub_3 = X_sub[cnd_3]\n",
    "X_sub_4 = X_sub[cnd_4]\n",
    "\n",
    "X_sub_1 = preproc(X_sub_1)\n",
    "X_sub_2 = preproc(X_sub_2)\n",
    "X_sub_3 = preproc(X_sub_3)\n",
    "X_sub_4 = preproc(X_sub_4)\n",
    "\n",
    "sub_1_pred = mlp_1.predict(X_sub_1)\n",
    "sub_2_pred = mlp_2.predict(X_sub_2)\n",
    "sub_3_pred = mlp_3.predict(X_sub_3)\n",
    "sub_4_pred = mlp_4.predict(X_sub_4)\n",
    "sub_1_pred = sub_1_pred*2 -1\n",
    "sub_2_pred = sub_2_pred*2 -1\n",
    "sub_3_pred = sub_3_pred*2 -1\n",
    "sub_4_pred = sub_4_pred*2 -1\n",
    "sub_pred = np.zeros(X_sub.shape[0])\n",
    "sub_pred[cnd_1] = sub_1_pred\n",
    "sub_pred[cnd_2] = sub_2_pred\n",
    "sub_pred[cnd_3] = sub_3_pred\n",
    "sub_pred[cnd_4] = sub_4_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.362277777973314"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, sub_pred, \"nn_4_split_final_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
