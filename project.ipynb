{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File_test = open('test.csv')\n",
    "File_train = open('train.csv')\n",
    "\n",
    "data_test = np.array(list(csv.reader(File_test)))\n",
    "data_train = np.array(list(csv.reader(File_train)))\n",
    "\n",
    "File_test.close()\n",
    "File_train.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    # converting -999. to nan to use np.nanmean and np.nanstd\n",
    "    X = np.where(X == -999., np.nan, X)\n",
    "    # standardizing the data Xd = (X_d - E[X_d])/(std(X_d))\n",
    "    X, means, stds = standardize(X)\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# converting the strings into floats and removing features names, labels and indexes\n",
    "X = np.array(data_train[1:,2:]).astype(np.float)\n",
    "\n",
    "num_train = 150000\n",
    "num_val = 50000\n",
    "num_test = 50000\n",
    "N = X.shape[0]\n",
    "assert num_train + num_val + num_test == N\n",
    "\n",
    "training_set = make_features(X[:num_train])\n",
    "validation_set = make_features(X[num_train:num_train+num_val])\n",
    "test_set = make_features(X[-num_test:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_set.shape)\n",
    "print(validation_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(data):\n",
    "    labels = np.array(data[1:,1])\n",
    "    return np.where(labels == 'b', 1, 0)\n",
    "    \n",
    "training_labels = make_labels(data_train[:num_train+1])\n",
    "validation_labels = make_labels(data_train[num_train:num_train+num_val+1])\n",
    "test_labels = make_labels(data_train[-num_test-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_labels.shape)\n",
    "print(validation_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "ws = []\n",
    "\n",
    "for gamma in np.linspace(0.1,1,10):\n",
    "    loss, w = least_squares_GD(validation_labels, validation_set, np.zeros(validation_set.shape[1]), 100, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ws[np.argmin(losses)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = training_set @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(training_labels - pred_tr)) / training_labels.shape[0]\n",
    "print(\"accuracy on training set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_set @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(test_labels - pred)) / test_labels.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1387578679999997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from implementations_clean import *\n",
    "from proj1_helpers import *\n",
    "\n",
    "y,X,ids = load_csv_data(\"train.csv\")\n",
    "#ADD BIAS\n",
    "\n",
    "import pandas as pd\n",
    "X = np.where(X == -999., np.nan, X)\n",
    "df = pd.DataFrame(X)\n",
    "df.head()\n",
    "\n",
    "#feature 1: correlations der_mass_MMC\n",
    "col_means = np.nanmean(X, axis=0)\n",
    "idxs = np.where(np.isnan(X))\n",
    "X[idxs] = np.take(col_means, idxs[1])\n",
    "X_gt_mmc = np.array(X[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X[:,0][X[:,0] > 140] = 140\n",
    "X = np.column_stack((X, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X[:,13]*np.cos(X[:,15])\n",
    "tau_py = X[:,13]*np.sin(X[:,15])\n",
    "tau_pz = X[:,13]*np.sinh(X[:,14])\n",
    "X = np.column_stack((X, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X[:,16]*np.cos(X[:,18])\n",
    "lep_py = X[:,16]*np.cos(X[:,18])\n",
    "lep_pz = X[:,16]*np.cos(X[:,17])\n",
    "X = np.column_stack((X, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X[:,22]*np.cos(X[:,24])\n",
    "jet_py = X[:,22]*np.cos(X[:,24])\n",
    "jet_pz = X[:,22]*np.cos(X[:,23])\n",
    "X = np.column_stack((X, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X[:,25]*np.cos(X[:,27])\n",
    "subjet_py = X[:,25]*np.cos(X[:,27])\n",
    "subjet_pz = X[:,25]*np.cos(X[:,26])\n",
    "X = np.column_stack((X, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "# feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X[:,11] = np.abs(X[:,11])\n",
    "#tau phi\n",
    "X[:,15] = np.abs(X[:,15])\n",
    "#lep phi\n",
    "X[:,18] = np.abs(X[:,18])\n",
    "#met phi\n",
    "X[:,20] = np.abs(X[:,20])\n",
    "#lead jet phi\n",
    "X[:,24] = np.abs(X[:,24])\n",
    "#sublead jet phi\n",
    "X[:,27] = np.abs(X[:,27])\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.head(20)\n",
    "X[:,11].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    0.0\n",
       "4    0.0\n",
       "Name: 22, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 22nd column shouln't be normalized and be expanded as one new colomn per discrete value\n",
    "df[22].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.655</td>\n",
       "      <td>27.980</td>\n",
       "      <td>30.297635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68.768</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-41.656246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.172</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-16.038136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81.417</td>\n",
       "      <td>0.414</td>\n",
       "      <td>22.645868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.915</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-17.299952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1       3          31\n",
       "0   51.655  27.980  30.297635\n",
       "1   68.768  48.146 -41.656246\n",
       "2  162.172  35.635 -16.038136\n",
       "3   81.417   0.414  22.645868\n",
       "4   16.915  16.405 -17.299952"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the columns with index 1, 3 and 31 have a high weight value in the first layer and could be expanded in polynomss\n",
    "df.iloc[:,[1,3,31]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.099192</td>\n",
       "      <td>0.068332</td>\n",
       "      <td>0.407680</td>\n",
       "      <td>-0.469966</td>\n",
       "      <td>-1.591638e+00</td>\n",
       "      <td>-1.153306e+00</td>\n",
       "      <td>1.806346e+00</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>1.033099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480884</td>\n",
       "      <td>-1.047625</td>\n",
       "      <td>-1.047625</td>\n",
       "      <td>-1.767430</td>\n",
       "      <td>-1.267320</td>\n",
       "      <td>-1.267320</td>\n",
       "      <td>-0.238641</td>\n",
       "      <td>0.122568</td>\n",
       "      <td>0.122568</td>\n",
       "      <td>-0.256095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.162178</td>\n",
       "      <td>0.552505</td>\n",
       "      <td>0.540136</td>\n",
       "      <td>-0.153167</td>\n",
       "      <td>-7.229012e-12</td>\n",
       "      <td>-6.304978e-12</td>\n",
       "      <td>6.751165e-13</td>\n",
       "      <td>1.404888</td>\n",
       "      <td>-0.756027</td>\n",
       "      <td>...</td>\n",
       "      <td>1.922023</td>\n",
       "      <td>0.997382</td>\n",
       "      <td>0.997382</td>\n",
       "      <td>0.463829</td>\n",
       "      <td>0.645996</td>\n",
       "      <td>0.645996</td>\n",
       "      <td>-0.652951</td>\n",
       "      <td>0.945345</td>\n",
       "      <td>0.945345</td>\n",
       "      <td>0.614340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.415335</td>\n",
       "      <td>3.195156</td>\n",
       "      <td>1.096560</td>\n",
       "      <td>-0.349710</td>\n",
       "      <td>-7.229012e-12</td>\n",
       "      <td>-6.304978e-12</td>\n",
       "      <td>6.751165e-13</td>\n",
       "      <td>0.989770</td>\n",
       "      <td>-0.430168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289035</td>\n",
       "      <td>1.635168</td>\n",
       "      <td>1.635168</td>\n",
       "      <td>1.751947</td>\n",
       "      <td>-0.612340</td>\n",
       "      <td>-0.612340</td>\n",
       "      <td>0.970068</td>\n",
       "      <td>-1.640389</td>\n",
       "      <td>-1.640389</td>\n",
       "      <td>-1.050357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.162178</td>\n",
       "      <td>0.910379</td>\n",
       "      <td>-0.005853</td>\n",
       "      <td>-0.903016</td>\n",
       "      <td>-7.229012e-12</td>\n",
       "      <td>-6.304978e-12</td>\n",
       "      <td>6.751165e-13</td>\n",
       "      <td>1.196690</td>\n",
       "      <td>-0.830735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.681156</td>\n",
       "      <td>-1.447571</td>\n",
       "      <td>-1.447571</td>\n",
       "      <td>0.933633</td>\n",
       "      <td>-0.130971</td>\n",
       "      <td>-0.130971</td>\n",
       "      <td>-0.015695</td>\n",
       "      <td>-0.004537</td>\n",
       "      <td>-0.004537</td>\n",
       "      <td>0.002805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.162178</td>\n",
       "      <td>-0.914556</td>\n",
       "      <td>1.313369</td>\n",
       "      <td>-0.651804</td>\n",
       "      <td>-7.229012e-12</td>\n",
       "      <td>-6.304978e-12</td>\n",
       "      <td>6.751165e-13</td>\n",
       "      <td>1.938794</td>\n",
       "      <td>-0.112795</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.504646</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.070223</td>\n",
       "      <td>-0.130971</td>\n",
       "      <td>-0.130971</td>\n",
       "      <td>-0.015695</td>\n",
       "      <td>-0.004537</td>\n",
       "      <td>-0.004537</td>\n",
       "      <td>0.002805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4             5             6   \\\n",
       "0  1.0  1.099192  0.068332  0.407680 -0.469966 -1.591638e+00 -1.153306e+00   \n",
       "1  1.0  1.162178  0.552505  0.540136 -0.153167 -7.229012e-12 -6.304978e-12   \n",
       "2  1.0  0.415335  3.195156  1.096560 -0.349710 -7.229012e-12 -6.304978e-12   \n",
       "3  1.0  1.162178  0.910379 -0.005853 -0.903016 -7.229012e-12 -6.304978e-12   \n",
       "4  1.0  1.162178 -0.914556  1.313369 -0.651804 -7.229012e-12 -6.304978e-12   \n",
       "\n",
       "             7         8         9   ...        34        35        36  \\\n",
       "0  1.806346e+00  0.882478  1.033099  ...  0.480884 -1.047625 -1.047625   \n",
       "1  6.751165e-13  1.404888 -0.756027  ...  1.922023  0.997382  0.997382   \n",
       "2  6.751165e-13  0.989770 -0.430168  ... -0.289035  1.635168  1.635168   \n",
       "3  6.751165e-13  1.196690 -0.830735  ... -0.681156 -1.447571 -1.447571   \n",
       "4  6.751165e-13  1.938794 -0.112795  ... -1.504646  0.001200  0.001200   \n",
       "\n",
       "         37        38        39        40        41        42        43  \n",
       "0 -1.767430 -1.267320 -1.267320 -0.238641  0.122568  0.122568 -0.256095  \n",
       "1  0.463829  0.645996  0.645996 -0.652951  0.945345  0.945345  0.614340  \n",
       "2  1.751947 -0.612340 -0.612340  0.970068 -1.640389 -1.640389 -1.050357  \n",
       "3  0.933633 -0.130971 -0.130971 -0.015695 -0.004537 -0.004537  0.002805  \n",
       "4  0.070223 -0.130971 -0.130971 -0.015695 -0.004537 -0.004537  0.002805  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = make_features(X)\n",
    "df = pd.DataFrame(X)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(0.8*((X.shape)[0]))\n",
    "X_train = X[:cutoff]\n",
    "y_train = y[:cutoff]\n",
    "X_test = X[cutoff:]\n",
    "y_test = y[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w, mse = True):\n",
    "    N = y.shape[0]\n",
    "    if mse:\n",
    "        e = y - tx @ w\n",
    "        loss = 1/(2 * N) * e.T @ e\n",
    "    else:\n",
    "        loss = np.mean(np.abs(y - tx @ w))\n",
    "    return loss\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis = 0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis = 0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backprop\n",
    "\n",
    "For MSE:\n",
    "\n",
    "$ \n",
    "    \\frac{\\delta L}{a_n} = \\frac{\\delta (a_n - y)^2}{\\delta a_{n}} = 2(a_n - y)  \\\\\n",
    "    \\frac{\\delta a_{i}}{\\delta z_{i}} = \\frac{\\delta S(z_{i})}{\\delta z_{i}} = S(z_{i})(1 - S(z_{i})) \\\\ \n",
    "    \\frac{\\delta z_{i+1}}{\\delta w_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta w_{i}} = a_{i} \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta b_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta b_{i}} = 1  \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta a_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta a_{i}}= w_{i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:    \n",
    "    #activations: 'relu', 'sigmoid', 'linear'\n",
    "    #loss assumed to be BCE\n",
    "    def __init__(self, lambda_ = 0.001,  dimensions = [2,10,1], activations = ['relu','sigmoid'] ,weight_decay = 0):\n",
    "        assert (len(dimensions)-1) == len(activations), \"Number of dimensions and activation functions do not match\"\n",
    "        # number of layers of our MLP\n",
    "        self.dimensions = dimensions\n",
    "        self.num_layers = len(dimensions)\n",
    "        self.lambda_ = lambda_\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        # the first layer is the input data\n",
    "        self.activations = {}\n",
    "        self.activations_grad = {}\n",
    "        \n",
    "        for n in np.arange(self.num_layers - 1):\n",
    "            # the weights are initialized acccording to a normal distribution and divided by the size of the layer they're on\n",
    "            self.weights[n + 1] = np.random.randn(dimensions[n + 1],dimensions[n]) / np.sqrt(dimensions[n])\n",
    "            # bias are all initialized to zero\n",
    "            self.bias[n + 1] = np.zeros(dimensions[n + 1])\n",
    "            \n",
    "            if activations[n] == 'relu':\n",
    "                self.activations[n+1] = self.relu\n",
    "                self.activations_grad[n+1] = self.relu_gradient\n",
    "            elif activations[n] == 'sigmoid':\n",
    "                self.activations[n+1] = self.sigmoid\n",
    "                self.activations_grad[n+1] = self.sigmoid_gradient\n",
    "            else:\n",
    "                self.activations[n+1] = lambda x : x\n",
    "                self.activations_grad[n+1] = lambda x : 1\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        \n",
    "        # keep track of all z and a to compute gradient in the backpropagation\n",
    "        z = {}\n",
    "        # the first layer is the input data\n",
    "        a = {1:x}\n",
    "        # We compute z[n+1] = a[n] * w[n] + b[n]\n",
    "        # and a[n+1] = f(z[n+1]) = f(a[n] * x[n] + b[n]) where * is the inner product\n",
    "\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            # we compute z_ and a_ for every sample and store them inside a single numpy array for every layer\n",
    "            # this actually may require some memory since we store a lot of data in two 3D matrix\n",
    "            z_ = np.zeros((batch_size, self.dimensions[n]))\n",
    "            a_ = np.zeros((batch_size, self.dimensions[n]))\n",
    "            for i in range(batch_size):\n",
    "                z_[i,:] = self.weights[n] @ a[n][i,:] + self.bias[n]\n",
    "                a_[i,:] = self.activations[n](z_[i,:])\n",
    "            z[n + 1] = z_\n",
    "            a[n + 1] = a_\n",
    "\n",
    "        # the prediction is the final layer\n",
    "        y_pred = a[n+1]\n",
    "        return y_pred, a, z\n",
    "    \n",
    "    # returns a prediction\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y_i_proba,_,_ = self.feed_forward(X[i].squeeze()) \n",
    "            preds[i] = (y_i_proba > 0.5)\n",
    "        return preds\n",
    "    \n",
    "    def back_propagate(self, y, y_pred, a, z):\n",
    "        \n",
    "        batch_size = y_pred.shape[0]\n",
    "        \n",
    "        weights_gradient = {}\n",
    "        bias_gradient = {}\n",
    "        \n",
    "        # base gradient of every sample (batch_size x 1)\n",
    "        nabla_n = self.BCE_gradient(y,y_pred)\n",
    "        \n",
    "        for n in np.flip(np.arange(1, self.num_layers)):\n",
    "            # the weights gradients we want to compute for every sample (dim_weights x batch_size)\n",
    "            weight_gradients = np.zeros((self.dimensions[n], self.dimensions[n-1], batch_size))\n",
    "            # the bias gradients we want to compute for every sample (dim_n x batch_size)\n",
    "            bias_gradients = np.zeros((self.dimensions[n], batch_size))\n",
    "            # the next nablas for every sample (batch_size x previous_dim)\n",
    "            nabla = np.zeros((batch_size, self.dimensions[n-1]))\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                # we compute the gradients of the weigths for every sample\n",
    "                nabla_ = nabla_n[i] * self.activations_grad[n]((z[n+1][i,:]))\n",
    "                weight_gradients[:,:,i] = np.outer(nabla_, a[n][i,:])\n",
    "                \n",
    "                # we compute the bias gradients for every sample\n",
    "                bias_gradients[:,i] = nabla_\n",
    "                # we compute the nabla for the next iteration\n",
    "                nabla[i] = np.dot(nabla_n[i], self.weights[n])\n",
    "                \n",
    "            nabla_n = nabla\n",
    "            # for both weights and bias we take the mean over all samples\n",
    "            weights_gradient[n] = np.mean(weight_gradients, axis = 2)\n",
    "            bias_gradient[n] = np.mean(bias_gradients, axis = 1)\n",
    "        \n",
    "        return weights_gradient, bias_gradient\n",
    "        ## self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "    \n",
    "    #weight decay : l2 reg\n",
    "    def gradient_descent_step(self, weights_gradient, bias_gradient):\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            self.weights[n] = self.weights[n] - self.lambda_ * (weights_gradient[n] + self.weight_decay*self.weights[n])\n",
    "            self.bias[n] = self.bias[n] - self.lambda_ * (bias_gradient[n] + self.weight_decay*self.bias[n])            \n",
    "    \n",
    "    #batch size = 1 for now\n",
    "    def train(self, X, y, max_iter, batch_size = 5):\n",
    "        for i in np.arange(max_iter):\n",
    "            idxs = np.random.randint(0, X.shape[0],batch_size)\n",
    "            X_batch = X[idxs].squeeze()\n",
    "            y_batch = y[idxs]\n",
    "            \n",
    "            y_pred, a, z = self.feed_forward(X_batch)\n",
    "            weights_gradient, bias_gradient = self.back_propagate(y_batch,y_pred,a, z)\n",
    "\n",
    "            self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "            \n",
    "            if (i % 1500 == 0):\n",
    "                loss = self.BCE_loss(X,y)\n",
    "                print(\"computing loss...\")\n",
    "                print(\"Iteration : {}, loss : {}\".format(i,loss))\n",
    "        loss = self.BCE_loss(X,y)\n",
    "        return loss\n",
    "            \n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_gradient(self,z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "    def relu(self,z):\n",
    "        return np.where(z < 0, 0, z)\n",
    "\n",
    "    def relu_gradient(self, z):\n",
    "        return np.where(z < 0, 0, 1)\n",
    "        \n",
    "    #check if possible to vectorize\n",
    "    def BCE_loss(self,X, y):\n",
    "        loss = 0\n",
    "        N = len(y)\n",
    "        for i in range(N):\n",
    "            y_pred,_,_ = self.feed_forward(X)\n",
    "            eps = 1e-7\n",
    "            loss_i = -(y[i]*np.log(y_pred+eps) + (1-y[i])*np.log(1-y_pred+eps))\n",
    "            loss = loss + loss_i/N\n",
    "        return loss\n",
    "    \n",
    "    def BCE_gradient(self,y,y_pred):\n",
    "        return y_pred.flatten()-y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = X_train.shape[1]\n",
    "n_h1 = 30\n",
    "n_h2 = 30\n",
    "n_h3 = 30\n",
    "out_dim = 1\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,out_dim]\n",
    "activations = ['relu','relu','relu','sigmoid']\n",
    "lambda_ = 0.001\n",
    "weight_decay = 0.001\n",
    "mlp = MLP(lambda_ = lambda_, dimensions = dimensions, activations = activations, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.train(X_train,y_train,max_iter = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_train)\n",
    "acc = 1-np.sum(np.abs(y_pred - y_train)) / X_train.shape[0]\n",
    "print(\"accuracy at training: {} % \".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)\n",
    "acc = 1-np.sum(np.abs(y_pred - y_test)) / X_test.shape[0]\n",
    "print(\"accuracy at testing: {} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
