{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (nan_predic.py, line 35)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\leose\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3296\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-17c2d5d357c0>\"\u001b[1;36m, line \u001b[1;32m5\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from nan_predic import *\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\leose\\Desktop\\ML_project\\nan_predic.py\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    predicted_masses = np.where(predicted_masses < np.min(y_mass), np.min(y_mass), predicted_masses)])\u001b[0m\n\u001b[1;37m                                                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from datetime import datetime\n",
    "from nan_predic import *\n",
    "np.random.seed(2)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X,ids = load_csv_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nan_predic(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.where(X == -999., np.nan, X)\n",
    "y_1 = y[~np.isnan(X).any(axis=1)]\n",
    "X_1 = X[~np.isnan(X).any(axis=1)]\n",
    "y_2 = y[np.isnan(X).any(axis=1)]\n",
    "X_2 = X[np.isnan(X).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_features(X):\n",
    "    X_copy = np.copy(X)\n",
    "    for i in range(X_copy.shape[1]):\n",
    "        for j in range(i,X_copy.shape[1]):\n",
    "            X = np.column_stack((X, X_copy[:,i]*X_copy[:,j]))\n",
    "    return X\n",
    "\n",
    "def make_features(X):\n",
    "    # converting -999. to nan to use np.nanmean and np.nanstd\n",
    "    X = np.where(X == -999., np.nan, X)\n",
    "    # standardizing the data Xd = (X_d - E[X_d])/(std(X_d))\n",
    "    X, means, stds = standardize(X)\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(X):\n",
    "    col_means = np.nanmean(X, axis=0)\n",
    "    col_means = np.nanmedian(X, axis=0)\n",
    "    idxs = np.where(np.isnan(X))\n",
    "    X[idxs] = np.take(col_means, idxs[1])\n",
    "\n",
    "    #feature 1: correlations der_mass_MMC\n",
    "    X_gt_mmc = np.array(X[:,0], copy=True)\n",
    "    # X_0_cop = np.array(X[:,0], copy=True)\n",
    "    X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "    # X = np.column_stack((X, X_gt_mmc))\n",
    "    X[:,0][X[:,0] > 140] = 140\n",
    "    X = np.column_stack((X, X_gt_mmc))\n",
    "\n",
    "    #feature 2: add momentums\n",
    "    #tau momentum\n",
    "    tau_px = X[:,13]*np.cos(X[:,15])\n",
    "    tau_py = X[:,13]*np.sin(X[:,15])\n",
    "    tau_pz = X[:,13]*np.sinh(X[:,14])\n",
    "    tau_mod = X[:,13]*np.cosh(X[:,14])\n",
    "    X = np.column_stack((X, tau_px,tau_py,tau_pz))\n",
    "    #lep momentum\n",
    "    lep_px = X[:,16]*np.cos(X[:,18])\n",
    "    lep_py = X[:,16]*np.sin(X[:,18])\n",
    "    lep_pz = X[:,16]*np.sinh(X[:,17])\n",
    "    lep_mod = X[:,16]*np.cosh(X[:,17])\n",
    "    X = np.column_stack((X, lep_px,lep_py,lep_pz))\n",
    "    #leading jet momentum\n",
    "    jet_px = X[:,23]*np.cos(X[:,25])\n",
    "    jet_py = X[:,23]*np.sin(X[:,25])\n",
    "    jet_pz = X[:,23]*np.sinh(X[:,24])\n",
    "    jet_mod = X[:,23]*np.cosh(X[:,24])\n",
    "    X = np.column_stack((X, jet_px,jet_py,jet_pz))\n",
    "    #subleading jet momentum\n",
    "    subjet_px = X[:,26]*np.cos(X[:,28])\n",
    "    subjet_py = X[:,26]*np.sin(X[:,28])\n",
    "    subjet_pz = X[:,26]*np.sinh(X[:,27])\n",
    "    subjet_mod = X[:,26]*np.cosh(X[:,27])\n",
    "    X = np.column_stack((X, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "    #feature 8: total invariant mass\n",
    "    term_1 = np.sqrt(tau_px**2 + tau_py**2 + tau_pz**2) + np.sqrt(lep_px**2 + lep_py**2 + lep_pz**2) \\\n",
    "    + np.sqrt(jet_px**2 + jet_py**2 + jet_pz**2) + np.sqrt(subjet_px**2 + subjet_py**2 + subjet_pz**2)\n",
    "    term_2 = (tau_px + lep_px + jet_px + subjet_px)**2 + (tau_py + lep_py + jet_py + subjet_py)**2 \\\n",
    "            + (tau_pz + lep_pz + jet_pz + subjet_pz)**2\n",
    "    inv_mass = np.sqrt(term_1**2 - term_2)\n",
    "\n",
    "\n",
    "    #feature 3: abs angles\n",
    "    #der_met_phi_centrality\n",
    "    X[:,11] = np.abs(X[:,11])\n",
    "    #tau phi\n",
    "    X[:,15] = np.abs(X[:,15])\n",
    "    #lep phi\n",
    "    X[:,18] = np.abs(X[:,18])\n",
    "    #met phi\n",
    "    X[:,20] = np.abs(X[:,20])\n",
    "    #lead jet phi\n",
    "    X[:,24] = np.abs(X[:,24])\n",
    "    #sublead jet phi\n",
    "    X[:,27] = np.abs(X[:,27])\n",
    "    #R sep abs\n",
    "    X[:,7] = np.abs(X[:,7])\n",
    "\n",
    "    #feature 9: log\n",
    "    inv_log_cols = (0,1,2,3,4,5,7,8,9,10,12,13,16,19,21,23,26)\n",
    "    X_inv_log_cols = np.log(1 / (1 + X[:, inv_log_cols]))\n",
    "    X = np.hstack((X, X_inv_log_cols))\n",
    "    # X_test_inv_log_cols = np.log(1 / (1 + X_test[:, inv_log_cols]))\n",
    "    # X_test = np.hstack((X_test, X_test_inv_log_cols))\n",
    "\n",
    "\n",
    "    #feature 4: categorical PRI_jet_num\n",
    "    jet_num_0 = (X[:,22] == 0).astype(int)\n",
    "    jet_num_1 = (X[:,22] == 1).astype(int)\n",
    "    jet_num_2 = (X[:,22] == 2).astype(int)\n",
    "    jet_num_3 = (X[:,22] == 3).astype(int)\n",
    "\n",
    "    # #feature 5: pt ratios\n",
    "    # #tau_lep_ratio = PRI_tau_pt/PRI_lep_pt\n",
    "    tau_lep_ratio = X[:,13]/X[:,16]\n",
    "    # #met_tot_ratio = PRI_met/PRI_met_sumet\n",
    "    met_tot_ratio = X[:,19]/X[:,21]\n",
    "    # X = np.column_stack((X, tau_lep_ratio,jets_ratio,met_tot_ratio))\n",
    "    X = np.column_stack((X, tau_lep_ratio,met_tot_ratio))\n",
    "\n",
    "    # #feature 6: jets_diff_angle\n",
    "    jets_diff_angle = np.cos(X[:,24]-X[:,27])\n",
    "    X = np.column_stack((X, jets_diff_angle))\n",
    "\n",
    "#     X = np.column_stack((X, inv_mass))\n",
    "    X = make_features(X)\n",
    "    X = np.column_stack((X, jet_num_0, jet_num_1, jet_num_2, jet_num_3))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leose\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\leose\\Desktop\\ML_project\\implementations.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = x / std_x\n",
      "C:\\Users\\leose\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: Mean of empty slice\n",
      "  \n",
      "C:\\Users\\leose\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:959: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n",
      "C:\\Users\\leose\\Desktop\\ML_project\\implementations.py:15: RuntimeWarning: Mean of empty slice\n",
      "  mean_x = np.nanmean(x, axis = 0)\n",
      "C:\\Users\\leose\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1628: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "X_1 = preproc(X_1)\n",
    "X_2 = preproc(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(X)\n",
    "cutoff_1 = int(0.8*((X_1.shape)[0]))\n",
    "cutoff_2 = int(0.8*((X_2.shape)[0]))\n",
    "X_1_train = X_1#[:cutoff_1]\n",
    "y_1_train = y_1#[:cutoff_1]\n",
    "X_1_test = X_1[cutoff_1:]\n",
    "y_1_test = y_1[cutoff_1:]\n",
    "X_2_train = X_2#[:cutoff_2]\n",
    "y_2_train = y_2#[:cutoff_2]\n",
    "X_2_test = X_2[cutoff_2:]\n",
    "y_2_test = y_2[cutoff_2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares\n",
    "\n",
    "## gradient descent Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "losses = []\n",
    "ws = []\n",
    "gammas = np.linspace(0.01,0.03,21)\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_GD(y_1_train, X_1_train, np.zeros(X_1_train.shape[1]), max_iter, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.argmin(losses)\n",
    "w = ws[index]\n",
    "loss = losses[index]\n",
    "gammas[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 74.72399341715138%, loss is of 0.17375074752477893\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_1_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_train - pred_tr)) / X_1_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 74.72849623559198 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_1_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_test - pred)) / X_1_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stochastic gradient descent least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 20000\n",
    "losses = []\n",
    "ws = []\n",
    "gammas = np.linspace(0.01,0.03,21)\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_SGD(y_1_train, X_1_train, np.zeros(X_1_train.shape[1]), max_iter, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.argmin(losses)\n",
    "w = ws[index]\n",
    "loss = losses[index]\n",
    "gammas[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 66.45612211583948%, loss is of 0.2622736619593481\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_1_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_train - pred_tr)) / X_1_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 66.45012992204677 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_1_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_test - pred)) / X_1_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = least_squares(y_1_train, X_1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = X_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_test - pred)) / X_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0.01, 0.1, 11)\n",
    "losses = []\n",
    "ws = []\n",
    "for lambda_ in lambdas:\n",
    "    w, loss = ridge_regression(y_1_train, X_1_train, lambda_)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for w in ws:\n",
    "    pred_test = X_1_test @ w\n",
    "    pred_test = np.where(pred_test > 1/2, 1, 0)\n",
    "    accuracy = 100 - 100 * np.sum(np.abs(y_1_test - pred_test)) / X_1_test.shape[0]\n",
    "    accuracies.append(accuracy)\n",
    "index = np.argmax(accuracies)\n",
    "w = ws[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 77.3264839726292%, loss is of 0.21451560123730073\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_1_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_train - pred_tr)) / X_1_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 77.40688919981345 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_1_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_test - pred)) / X_1_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 20000\n",
    "gamma = 0.005\n",
    "w, loss = logistic_regression(y_1_train, X_1_train, np.zeros(X_1_train.shape[1]), max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 75.2077128598746%, loss is of 0.48639858062277364\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_1_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_train - pred_tr)) / X_1_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 75.40475714571258 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_1_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_test - pred)) / X_1_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reg logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 20000\n",
    "gamma = 0.005\n",
    "lambda_ = 0\n",
    "w, loss = reg_logistic_regression(y_1_train, X_1_train, lambda_, np.zeros(X_1_train.shape[1]), max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = X_1_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_train - pred_tr)) / X_1_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = X_1_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_1_test - pred)) / X_1_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backprop\n",
    "\n",
    "For MSE:\n",
    "\n",
    "$ \n",
    "    \\frac{\\delta L}{a_n} = \\frac{\\delta (a_n - y)^2}{\\delta a_{n}} = 2(a_n - y)  \\\\\n",
    "    \\frac{\\delta a_{i}}{\\delta z_{i}} = \\frac{\\delta S(z_{i})}{\\delta z_{i}} = S(z_{i})(1 - S(z_{i})) \\\\ \n",
    "    \\frac{\\delta z_{i+1}}{\\delta w_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta w_{i}} = a_{i} \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta b_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta b_{i}} = 1  \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta a_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta a_{i}}= w_{i} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tryin' ma best to vectorize baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.where(z < 0, 0, z)\n",
    "\n",
    "def relu_gradient(z):\n",
    "    return np.where(z < 0, 0, 1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def grad_loss(y_pred, y):\n",
    "    return y_pred - y\n",
    "\n",
    "def BCE_gradient(y,y_pred):\n",
    "    #return y_pred-y\n",
    "    return (-y/y_pred + (1-y)/(1-y_pred))\n",
    "\n",
    "class layer:\n",
    "\n",
    "    def __init__(self, dim_0, dim_1, activation):\n",
    "        self.w = np.random.randn(dim_0, dim_1) / np.sqrt(dim_0)\n",
    "        self.b = np.zeros(dim_0)\n",
    "        if activation == 'relu':\n",
    "            self.f = relu\n",
    "            self.f_grad = relu_gradient\n",
    "        if activation == 'sigmoid':\n",
    "            self.f = sigmoid\n",
    "            self.f_grad = sigmoid_gradient\n",
    "\n",
    "    def feed_forward(self, a):\n",
    "        self.a_prev = a\n",
    "        if len(a.shape) == 1:\n",
    "            # for batch_size 1\n",
    "            self.z = np.dot(self.w, a) + self.b\n",
    "        else:\n",
    "            self.z = np.dot(self.w, a) + np.tile(self.b, (a.shape[1],1)).T\n",
    "        self.a = self.f(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def back_propagate(self, grad):\n",
    "        grad = grad * self.f_grad(self.z)\n",
    "        self.w_grad = grad @ self.a_prev.T\n",
    "        self.b_grad = np.sum(grad, axis = 1)\n",
    "        return (grad.T @ self.w).T\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    # trying a vectorized MLP\n",
    "    def __init__(self, dim, activations):\n",
    "        \n",
    "        self.layers = []\n",
    "        for n in range(len(dim)-1):\n",
    "            self.layers.append(layer(dim[n + 1], dim[n], activations[n]))\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        a = X\n",
    "        for l in self.layers:\n",
    "            a = l.feed_forward(a)\n",
    "        return a\n",
    "    \n",
    "    def back_propagate(self, y_pred, y):\n",
    "        grad = BCE_gradient(y, y_pred)\n",
    "        for l in np.flip(self.layers):\n",
    "            grad = l.back_propagate(grad)\n",
    "        \n",
    "            \n",
    "    def gradient_descent_step(self, gamma, weight_decay):\n",
    "        for l in self.layers:\n",
    "            l.w -= (l.w_grad + l.w * weight_decay) * gamma\n",
    "            l.b -= (l.b_grad + l.b * weight_decay) * gamma\n",
    "            \n",
    "    def train(self, X, Y, batch_size, max_iter, gamma, weight_decay, number_of_loss_computations = 5):\n",
    "        \n",
    "        start = datetime.now()\n",
    "        gamma = gamma\n",
    "        div = int(max_iter / number_of_loss_computations)\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            \n",
    "            if i == int(max_iter / 3):\n",
    "                gamma = 0.1 * gamma\n",
    "            if i == int(max_iter * 0.75):\n",
    "                gamma = 0.1 * gamma\n",
    "            if i % div == 0:\n",
    "                print(\"{}% of the way\".format(int(i/max_iter * 100)))\n",
    "                print(self.BCE_loss(X, Y))\n",
    "                \n",
    "            idxs = np.arange(X.shape[0])\n",
    "            np.random.shuffle(idxs)\n",
    "            idxs = idxs[:batch_size]\n",
    "            \n",
    "            X_batch = X[idxs]\n",
    "            y_batch = y[idxs]\n",
    "            y_pred = self.feed_forward(X_batch.T)\n",
    "            self.back_propagate(y_pred, y_batch)\n",
    "            self.gradient_descent_step(gamma / batch_size, weight_decay)\n",
    "\n",
    "        end = datetime.now()\n",
    "        print(\"time taken:\", end - start)\n",
    "                \n",
    "    def BCE_loss(self,X, y):\n",
    "        loss = 0\n",
    "        N = len(y)\n",
    "        for i in range(N):\n",
    "            y_pred = self.feed_forward(X[i])\n",
    "            eps = 1e-7\n",
    "            loss_i = -(y[i]*np.log(y_pred+eps) + (1-y[i])*np.log(1-y_pred+eps))\n",
    "            loss = loss + loss_i/N\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def BCE_loss_vect(self, X, y):\n",
    "        y_pred = self.feed_forward(X.T)\n",
    "        return np.mean(y_pred * np.log(y_pred + eps) + (1 - y_pred) * np.log(1 - y_pred + eps))\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = self.feed_forward(X.T)\n",
    "        return np.where(y < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "in_dim = X_train.shape[1]\n",
    "n_h1 = 100\n",
    "n_h2 = 100\n",
    "n_h3 = 100\n",
    "n_h4 = 100\n",
    "n_h5 = 100\n",
    "n_h6 = 100\n",
    "n_h7 = 100\n",
    "out_dim = 1\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,n_h4,n_h5,n_h6,n_h7,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','relu','relu','sigmoid']\n",
    "\n",
    "# mlp = MLP(gamma = gamma, dimensions = dimensions, activations = activations, weight_decay = weight_decay)\n",
    "\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,n_h4,n_h5,n_h6,n_h7,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','relu','relu','sigmoid']\n",
    "mlp_1 = MLP(dimensions, activations)\n",
    "dimensions = [in_dim, 30,30,30,out_dim]\n",
    "activations = ['relu','relu','relu','sigmoid']\n",
    "mlp_2 = MLP(dimensions, activations)\n",
    "dimensions = [in_dim, 50,50,50,50,50,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','sigmoid']\n",
    "mlp_3 = MLP(dimensions, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.001\n",
    "weight_decay = 0.001\n",
    "max_iter = 1750000\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1.train(X, y, batch_size, max_iter, gamma, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = mlp_1.feed_forward(X_train.T)\n",
    "acc_train_1 = 1-np.sum(np.abs(y_pred1 - y_train)) / X_train.shape[0]\n",
    "y_pred1 = mlp_1.feed_forward(X_test.T)\n",
    "acc_test_1 = 1-np.sum(np.abs(y_pred1 - y_test)) / X_test.shape[0]\n",
    "print(\"first MLP\")\n",
    "print(\"training accuracy: {}% | test accuracy: {}%\".format(acc_train_1 * 100, acc_test_1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2.train(X, y, batch_size, max_iter, gamma, weight_decay * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = mlp_2.feed_forward(X_train.T)\n",
    "acc_train_2 = 1-np.sum(np.abs(y_pred2 - y_train)) / X_train.shape[0]\n",
    "y_pred2 = mlp_1.feed_forward(X_test.T)\n",
    "acc_test_2 = 1-np.sum(np.abs(y_pred2 - y_test)) / X_test.shape[0]\n",
    "print(\"second MLP\")\n",
    "print(\"training accuracy: {}% | test accuracy: {}%\".format(acc_train_2 * 100, acc_test_2 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3.train(X, y, batch_size, max_iter, gamma, weight_decay * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = mlp_3.feed_forward(X_train.T)\n",
    "acc_train_3 = 1-np.sum(np.abs(y_pred3 - y_train)) / X_train.shape[0]\n",
    "y_pred3 = mlp_3.feed_forward(X_test.T)\n",
    "acc_test_3 = 1-np.sum(np.abs(y_pred3 - y_test)) / X_test.shape[0]\n",
    "print(\"third MLP\")\n",
    "print(\"training accuracy: {}% | test accuracy: {}%\".format(acc_train_3 * 100, acc_test_3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,X_sub,ids = load_csv_data(\"test.csv\")\n",
    "#feature 1: correlations der_mass_MMC\n",
    "X_sub = np.where(X_sub == -999., np.nan, X_sub)\n",
    "col_means = np.nanmean(X_sub, axis=0)\n",
    "idxs = np.where(np.isnan(X_sub))\n",
    "X_sub[idxs] = np.take(col_means, idxs[1])\n",
    "X_gt_mmc = np.array(X_sub[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X_sub[:,0][X_sub[:,0] > 140] = 140\n",
    "X_sub = np.column_stack((X_sub, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X_sub[:,13]*np.cos(X_sub[:,15])\n",
    "tau_py = X_sub[:,13]*np.sin(X_sub[:,15])\n",
    "tau_pz = X_sub[:,13]*np.sinh(X_sub[:,14])\n",
    "X_sub = np.column_stack((X_sub, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X_sub[:,16]*np.cos(X_sub[:,18])\n",
    "lep_py = X_sub[:,16]*np.cos(X_sub[:,18])\n",
    "lep_pz = X_sub[:,16]*np.cos(X_sub[:,17])\n",
    "X_sub = np.column_stack((X_sub, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X_sub[:,22]*np.cos(X_sub[:,24])\n",
    "jet_py = X_sub[:,22]*np.cos(X_sub[:,24])\n",
    "jet_pz = X_sub[:,22]*np.cos(X_sub[:,23])\n",
    "X_sub = np.column_stack((X_sub, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X_sub[:,25]*np.cos(X_sub[:,27])\n",
    "subjet_py = X_sub[:,25]*np.cos(X_sub[:,27])\n",
    "subjet_pz = X_sub[:,25]*np.cos(X_sub[:,26])\n",
    "X_sub = np.column_stack((X_sub, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "# feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X_sub[:,11] = np.abs(X_sub[:,11])\n",
    "#tau phi\n",
    "X_sub[:,15] = np.abs(X_sub[:,15])\n",
    "#lep phi\n",
    "X_sub[:,18] = np.abs(X_sub[:,18])\n",
    "#met phi\n",
    "X_sub[:,20] = np.abs(X_sub[:,20])\n",
    "#lead jet phi\n",
    "X_sub[:,24] = np.abs(X_sub[:,24])\n",
    "#sublead jet phi\n",
    "X_sub[:,27] = np.abs(X_sub[:,27])\n",
    "\n",
    "X_sub = make_features(X_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_1 = mlp_1.feed_forward(X_sub.T)\n",
    "p_2 = mlp_2.feed_forward(X_sub.T)\n",
    "p_3 = mlp_3.feed_forward(X_sub.T)\n",
    "p = np.mean((p_1,p_2,p_3),axis = 0)\n",
    "p = p > 0.5\n",
    "sub_pred = p*2 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = sub_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, sub_pred, \"submission_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv(\"submission_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv(\"submission_test.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "\n",
    "y,X,ids = load_csv_data(\"train.csv\")\n",
    "\n",
    "X = np.where(X == -999., np.nan, X)\n",
    "X_ = X\n",
    "\n",
    "import pandas as pd\n",
    "mass_nan_index = [0]\n",
    "jet_nan_indexes = [4,5,6,12,26,27,28]\n",
    "jet_sub_nan_indexes = [23,24,25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### masses prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noNans = np.delete(X_, np.concatenate((jet_nan_indexes, jet_sub_nan_indexes)), 1)\n",
    "\n",
    "defined_mass_idx = np.where(np.isfinite(X_noNans[:,0])) #indexes of samples with a mass\n",
    "undefined_mass_idx = np.where(np.isnan(X_noNans[:,0]))  #indexes of samples with a missing mass\n",
    "\n",
    "X_mass = X_noNans[defined_mass_idx][:,1:] # we select the X of training set\n",
    "y_mass = X_noNans[defined_mass_idx][:,:1] # we select the y of the taining set\n",
    "\n",
    "X_mass,_,_ = standardize(X_mass) # we standardize both \n",
    "y_mass,_,_ = standardize(y_mass)\n",
    "y_mass = y_mass.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.ones(X_mass.shape[1]) / X_mass.shape[1]\n",
    "lambda_ = 0.0001\n",
    "max_iter = 1000\n",
    "gamma = 0.005\n",
    "w, loss = least_squares_GD(y_mass, X_mass, initial_w, max_iter, gamma) # since it isn't classification, we use least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict_mass,_,_ = standardize(X_noNans[undefined_mass_idx][:,1:]) # we select the set to be predicted\n",
    "predicted_masses = to_predict_mass @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = np.zeros(X_.shape[0])\n",
    "masses[defined_mass_idx] = y_mass\n",
    "masses[undefined_mass_idx] = predicted_masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_[:,mass_nan_index[0]] = masses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jet predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noNans = np.delete(X_, np.concatenate((mass_nan_index, jet_sub_nan_indexes)), 1)\n",
    "\n",
    "defined_jet_idx = np.where(np.isfinite(X_[:,jet_nan_indexes[0]]))[0] #indexes of samples with jets\n",
    "undefined_jet_idx = np.where(np.isnan(X_[:,jet_nan_indexes[0]]))[0]  #indexes of samples with missing jets\n",
    "\n",
    "X_jet = np.delete(X_, np.concatenate((mass_nan_index, jet_nan_indexes, jet_sub_nan_indexes)), 1)[defined_jet_idx] # we select the X of training set\n",
    "to_predict_jets = np.delete(X_, np.concatenate((mass_nan_index, jet_nan_indexes, jet_sub_nan_indexes)), 1)[undefined_jet_idx]\n",
    "y_jet_1 = X_[defined_jet_idx][:,jet_nan_indexes[0]] # we select the y of the taining set [4]\n",
    "y_jet_2 = X_[defined_jet_idx][:,jet_nan_indexes[1]] # we select the y of the taining set [5]\n",
    "y_jet_3 = X_[defined_jet_idx][:,jet_nan_indexes[2]] # we select the y of the taining set [6]\n",
    "y_jet_4 = X_[defined_jet_idx][:,jet_nan_indexes[3]] # we select the y of the taining set [12]\n",
    "y_jet_5 = X_[defined_jet_idx][:,jet_nan_indexes[4]] # we select the y of the taining set [26]\n",
    "y_jet_6 = X_[defined_jet_idx][:,jet_nan_indexes[5]] # we select the y of the taining set [27]\n",
    "y_jet_7 = X_[defined_jet_idx][:,jet_nan_indexes[6]] # we select the y of the taining set [28]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jet,_,_ = standardize(X_jet) # we standardize\n",
    "to_predict_jets,_,_ = standardize(to_predict_jets)\n",
    "y_jet_1,_,_ = standardize(y_jet_1)\n",
    "y_jet_2,_,_ = standardize(y_jet_2)\n",
    "y_jet_3,_,_ = standardize(y_jet_3)\n",
    "y_jet_4,_,_ = standardize(y_jet_4)\n",
    "y_jet_5,_,_ = standardize(y_jet_5)\n",
    "y_jet_6,_,_ = standardize(y_jet_6)\n",
    "y_jet_7,_,_ = standardize(y_jet_7)\n",
    "\n",
    "y_jet_1 = y_jet_1.squeeze()\n",
    "y_jet_2 = y_jet_2.squeeze()\n",
    "y_jet_3 = y_jet_3.squeeze()\n",
    "y_jet_4 = y_jet_4.squeeze()\n",
    "y_jet_5 = y_jet_5.squeeze()\n",
    "y_jet_6 = y_jet_6.squeeze()\n",
    "y_jet_7 = y_jet_7.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_jets = [y_jet_1, y_jet_2, y_jet_3, y_jet_4, y_jet_4, y_jet_5, y_jet_6, y_jet_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.0001\n",
    "max_iter = 1000\n",
    "gamma = 0.005\n",
    "initial_w = np.ones(X_jet.shape[1]) / X_jet.shape[1]\n",
    "\n",
    "ws = []\n",
    "losses = []\n",
    "\n",
    "for y_jet in y_jets:\n",
    "    w, loss = least_squares_GD(y_jet, X_jet, initial_w, max_iter, gamma) # since it isn't classification, we use least_squares\n",
    "    ws.append(w)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_predictions = []\n",
    "for w in ws:\n",
    "    jet_prediction = to_predict_jets @ w\n",
    "    jet_predictions.append(jet_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(jet_nan_indexes)):\n",
    "    jet_col = np.zeros(X_.shape[0])\n",
    "    jet_col[defined_jet_idx] = y_jets[i]\n",
    "    jet_col[undefined_jet_idx] = jet_predictions[i]\n",
    "    X_[:, jet_nan_indexes[i]] = jet_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
