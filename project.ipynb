{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File_test = open('test.csv')\n",
    "File_train = open('train.csv')\n",
    "\n",
    "data_test = np.array(list(csv.reader(File_test)))\n",
    "data_train = np.array(list(csv.reader(File_train)))\n",
    "\n",
    "File_test.close()\n",
    "File_train.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    # converting -999. to nan to use np.nanmean and np.nanstd\n",
    "    X = np.where(X == -999., np.nan, X)\n",
    "    # standardizing the data Xd = (X_d - E[X_d])/(std(X_d))\n",
    "    X, means, stds = standardize(X)\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# converting the strings into floats and removing features names, labels and indexes\n",
    "X = np.array(data_train[1:,2:]).astype(np.float)\n",
    "\n",
    "num_train = 150000\n",
    "num_val = 50000\n",
    "num_test = 50000\n",
    "N = X.shape[0]\n",
    "assert num_train + num_val + num_test == N\n",
    "\n",
    "training_set = make_features(X[:num_train])\n",
    "validation_set = make_features(X[num_train:num_train+num_val])\n",
    "test_set = make_features(X[-num_test:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_set.shape)\n",
    "print(validation_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(data):\n",
    "    labels = np.array(data[1:,1])\n",
    "    return np.where(labels == 'b', 1, 0)\n",
    "    \n",
    "training_labels = make_labels(data_train[:num_train+1])\n",
    "validation_labels = make_labels(data_train[num_train:num_train+num_val+1])\n",
    "test_labels = make_labels(data_train[-num_test-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_labels.shape)\n",
    "print(validation_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "ws = []\n",
    "\n",
    "for gamma in np.linspace(0.1,1,10):\n",
    "    loss, w = least_squares_GD(validation_labels, validation_set, np.zeros(validation_set.shape[1]), 100, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ws[np.argmin(losses)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = training_set @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(training_labels - pred_tr)) / training_labels.shape[0]\n",
    "print(\"accuracy on training set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_set @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(test_labels - pred)) / test_labels.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations_clean import *\n",
    "from proj1_helpers import *\n",
    "\n",
    "y,X,ids = load_csv_data(\"train.csv\")\n",
    "#ADD BIAS\n",
    "\n",
    "import pandas as pd\n",
    "X = np.where(X == -999., np.nan, X)\n",
    "df = pd.DataFrame(X)\n",
    "df.head()\n",
    "\n",
    "#feature 1: correlations der_mass_MMC\n",
    "col_means = np.nanmean(X, axis=0)\n",
    "idxs = np.where(np.isnan(X))\n",
    "X[idxs] = np.take(col_means, idxs[1])\n",
    "X_gt_mmc = np.array(X[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X[:,0][X[:,0] > 140] = 140\n",
    "X = np.column_stack((X, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X[:,13]*np.cos(X[:,15])\n",
    "tau_py = X[:,13]*np.sin(X[:,15])\n",
    "tau_pz = X[:,13]*np.sinh(X[:,14])\n",
    "X = np.column_stack((X, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X[:,16]*np.cos(X[:,18])\n",
    "lep_py = X[:,16]*np.cos(X[:,18])\n",
    "lep_pz = X[:,16]*np.cos(X[:,17])\n",
    "X = np.column_stack((X, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X[:,22]*np.cos(X[:,24])\n",
    "jet_py = X[:,22]*np.cos(X[:,24])\n",
    "jet_pz = X[:,22]*np.cos(X[:,23])\n",
    "X = np.column_stack((X, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X[:,25]*np.cos(X[:,27])\n",
    "subjet_py = X[:,25]*np.cos(X[:,27])\n",
    "subjet_pz = X[:,25]*np.cos(X[:,26])\n",
    "X = np.column_stack((X, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "# feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X[:,11] = np.abs(X[:,11])\n",
    "#tau phi\n",
    "X[:,15] = np.abs(X[:,15])\n",
    "#lep phi\n",
    "X[:,18] = np.abs(X[:,18])\n",
    "#met phi\n",
    "X[:,20] = np.abs(X[:,20])\n",
    "#lead jet phi\n",
    "X[:,24] = np.abs(X[:,24])\n",
    "#sublead jet phi\n",
    "X[:,27] = np.abs(X[:,27])\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.head(20)\n",
    "X[:,11].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 22nd column shouln't be normalized and be expanded as one new colomn per discrete value\n",
    "df[22].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the columns with index 1, 3 and 31 have a high weight value in the first layer and could be expanded in polynomss\n",
    "df.iloc[:,[1,3,31]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_features(X)\n",
    "df = pd.DataFrame(X)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(0.8*((X.shape)[0]))\n",
    "X_train = X[:cutoff]\n",
    "y_train = y[:cutoff]\n",
    "X_test = X[cutoff:]\n",
    "y_test = y[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w, mse = True):\n",
    "    N = y.shape[0]\n",
    "    if mse:\n",
    "        e = y - tx @ w\n",
    "        loss = 1/(2 * N) * e.T @ e\n",
    "    else:\n",
    "        loss = np.mean(np.abs(y - tx @ w))\n",
    "    return loss\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis = 0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis = 0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backprop\n",
    "\n",
    "For MSE:\n",
    "\n",
    "$ \n",
    "    \\frac{\\delta L}{a_n} = \\frac{\\delta (a_n - y)^2}{\\delta a_{n}} = 2(a_n - y)  \\\\\n",
    "    \\frac{\\delta a_{i}}{\\delta z_{i}} = \\frac{\\delta S(z_{i})}{\\delta z_{i}} = S(z_{i})(1 - S(z_{i})) \\\\ \n",
    "    \\frac{\\delta z_{i+1}}{\\delta w_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta w_{i}} = a_{i} \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta b_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta b_{i}} = 1  \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta a_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta a_{i}}= w_{i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:    \n",
    "    #activations: 'relu', 'sigmoid', 'linear'\n",
    "    #loss assumed to be BCE\n",
    "    def __init__(self, lambda_ = 0.001,  dimensions = [2,10,1], activations = ['relu','sigmoid'] ,weight_decay = 0):\n",
    "        assert (len(dimensions)-1) == len(activations), \"Number of dimensions and activation functions do not match\"\n",
    "        # number of layers of our MLP\n",
    "        self.dimensions = dimensions\n",
    "        self.num_layers = len(dimensions)\n",
    "        self.lambda_ = lambda_\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        # the first layer is the input data\n",
    "        self.activations = {}\n",
    "        self.activations_grad = {}\n",
    "        \n",
    "        for n in np.arange(self.num_layers - 1):\n",
    "            # the weights are initialized acccording to a normal distribution and divided by the size of the layer they're on\n",
    "            self.weights[n + 1] = np.random.randn(dimensions[n + 1],dimensions[n]) / np.sqrt(dimensions[n])\n",
    "            # bias are all initialized to zero\n",
    "            self.bias[n + 1] = np.zeros(dimensions[n + 1])\n",
    "            \n",
    "            if activations[n] == 'relu':\n",
    "                self.activations[n+1] = self.relu\n",
    "                self.activations_grad[n+1] = self.relu_gradient\n",
    "            elif activations[n] == 'sigmoid':\n",
    "                self.activations[n+1] = self.sigmoid\n",
    "                self.activations_grad[n+1] = self.sigmoid_gradient\n",
    "            else:\n",
    "                self.activations[n+1] = lambda x : x\n",
    "                self.activations_grad[n+1] = lambda x : 1\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        \n",
    "        # keep track of all z and a to compute gradient in the backpropagation\n",
    "        z = {}\n",
    "        # the first layer is the input data\n",
    "        a = {1:x}\n",
    "        # We compute z[n+1] = a[n] * w[n] + b[n]\n",
    "        # and a[n+1] = f(z[n+1]) = f(a[n] * x[n] + b[n]) where * is the inner product\n",
    "\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            # we compute z_ and a_ for every sample and store them inside a single numpy array for every layer\n",
    "            # this actually may require some memory since we store a lot of data in two 3D matrix\n",
    "            z_ = np.zeros((batch_size, self.dimensions[n]))\n",
    "            a_ = np.zeros((batch_size, self.dimensions[n]))\n",
    "            for i in range(batch_size):\n",
    "                z_[i,:] = self.weights[n] @ a[n][i,:] + self.bias[n]\n",
    "                a_[i,:] = self.activations[n](z_[i,:])\n",
    "            z[n + 1] = z_\n",
    "            a[n + 1] = a_\n",
    "\n",
    "        # the prediction is the final layer\n",
    "        y_pred = a[n+1]\n",
    "        return y_pred, a, z\n",
    "    \n",
    "    # returns a prediction\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y_i_proba,_,_ = self.feed_forward(X[i].squeeze()) \n",
    "            preds[i] = (y_i_proba > 0.5)\n",
    "        return preds\n",
    "    \n",
    "    def back_propagate(self, y, y_pred, a, z):\n",
    "        \n",
    "        batch_size = y_pred.shape[0]\n",
    "        \n",
    "        weights_gradient = {}\n",
    "        bias_gradient = {}\n",
    "        \n",
    "        # base gradient of every sample (batch_size x 1)\n",
    "        nabla_n = self.BCE_gradient(y,y_pred)\n",
    "        \n",
    "        for n in np.flip(np.arange(1, self.num_layers)):\n",
    "            # the weights gradients we want to compute for every sample (dim_weights x batch_size)\n",
    "            weight_gradients = np.zeros((self.dimensions[n], self.dimensions[n-1], batch_size))\n",
    "            # the bias gradients we want to compute for every sample (dim_n x batch_size)\n",
    "            bias_gradients = np.zeros((self.dimensions[n], batch_size))\n",
    "            # the next nablas for every sample (batch_size x previous_dim)\n",
    "            nabla = np.zeros((batch_size, self.dimensions[n-1]))\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                # we compute the gradients of the weigths for every sample\n",
    "                nabla_ = nabla_n[i] * self.activations_grad[n]((z[n+1][i,:]))\n",
    "                weight_gradients[:,:,i] = np.outer(nabla_, a[n][i,:])\n",
    "                \n",
    "                # we compute the bias gradients for every sample\n",
    "                bias_gradients[:,i] = nabla_\n",
    "                # we compute the nabla for the next iteration\n",
    "                nabla[i] = np.dot(nabla_n[i], self.weights[n])\n",
    "                \n",
    "            nabla_n = nabla\n",
    "            # for both weights and bias we take the mean over all samples\n",
    "            weights_gradient[n] = np.mean(weight_gradients, axis = 2)\n",
    "            bias_gradient[n] = np.mean(bias_gradients, axis = 1)\n",
    "        \n",
    "        return weights_gradient, bias_gradient\n",
    "        ## self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "    \n",
    "    #weight decay : l2 reg\n",
    "    def gradient_descent_step(self, weights_gradient, bias_gradient):\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            self.weights[n] = self.weights[n] - self.lambda_ * (weights_gradient[n] + self.weight_decay*self.weights[n])\n",
    "            self.bias[n] = self.bias[n] - self.lambda_ * (bias_gradient[n] + self.weight_decay*self.bias[n])            \n",
    "    \n",
    "    #batch size = 1 for now\n",
    "    def train(self, X, y, max_iter, batch_size = 5):\n",
    "        for i in np.arange(max_iter):\n",
    "            idxs = np.random.randint(0, X.shape[0],batch_size)\n",
    "            X_batch = X[idxs].squeeze()\n",
    "            y_batch = y[idxs]\n",
    "            \n",
    "            y_pred, a, z = self.feed_forward(X_batch)\n",
    "            weights_gradient, bias_gradient = self.back_propagate(y_batch,y_pred,a, z)\n",
    "\n",
    "            self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "            \n",
    "            if (i % 1500 == 0):\n",
    "                loss = self.BCE_loss(X,y)\n",
    "                print(\"computing loss...\")\n",
    "                print(\"Iteration : {}, loss : {}\".format(i,loss))\n",
    "        loss = self.BCE_loss(X,y)\n",
    "        return loss\n",
    "            \n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_gradient(self,z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "    def relu(self,z):\n",
    "        return np.where(z < 0, 0, z)\n",
    "\n",
    "    def relu_gradient(self, z):\n",
    "        return np.where(z < 0, 0, 1)\n",
    "        \n",
    "    #check if possible to vectorize\n",
    "    def BCE_loss(self,X, y):\n",
    "        loss = 0\n",
    "        N = len(y)\n",
    "        for i in range(N):\n",
    "            y_pred,_,_ = self.feed_forward(X)\n",
    "            eps = 1e-7\n",
    "            loss_i = -(y[i]*np.log(y_pred+eps) + (1-y[i])*np.log(1-y_pred+eps))\n",
    "            loss = loss + loss_i/N\n",
    "        return loss\n",
    "    \n",
    "    def BCE_gradient(self,y,y_pred):\n",
    "        return y_pred.flatten()-y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = X_train.shape[1]\n",
    "n_h1 = 30\n",
    "n_h2 = 30\n",
    "n_h3 = 30\n",
    "out_dim = 1\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,out_dim]\n",
    "activations = ['relu','relu','relu','sigmoid']\n",
    "lambda_ = 0.001\n",
    "weight_decay = 0.001\n",
    "mlp = MLP(lambda_ = lambda_, dimensions = dimensions, activations = activations, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.train(X_train,y_train,max_iter = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_train)\n",
    "acc = 1-np.sum(np.abs(y_pred - y_train)) / X_train.shape[0]\n",
    "print(\"accuracy at training: {} % \".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)\n",
    "acc = 1-np.sum(np.abs(y_pred - y_test)) / X_test.shape[0]\n",
    "print(\"accuracy at testing: {} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tryin' ma best to vectorize baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3     4        5      6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980  0.91  124.711  2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146   NaN      NaN    NaN  3.473   2.078   \n",
       "2      NaN  162.172  125.953  35.635   NaN      NaN    NaN  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414   NaN      NaN    NaN  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405   NaN      NaN    NaN  3.891  16.405   \n",
       "\n",
       "        9   ...     20       21   22      23     24     25      26    27  \\\n",
       "0  197.760  ... -0.277  258.733  2.0  67.435  2.150  0.444  46.062  1.24   \n",
       "1  125.157  ... -1.916  164.546  1.0  46.226  0.725  1.158     NaN   NaN   \n",
       "2  197.814  ... -2.186  260.414  1.0  44.251  2.053 -2.028     NaN   NaN   \n",
       "3   75.968  ...  0.060   86.062  0.0     NaN    NaN    NaN     NaN   NaN   \n",
       "4   57.983  ... -0.871   53.131  0.0     NaN    NaN    NaN     NaN   NaN   \n",
       "\n",
       "      28       29  \n",
       "0 -2.475  113.497  \n",
       "1    NaN   46.226  \n",
       "2    NaN   44.251  \n",
       "3    NaN    0.000  \n",
       "4    NaN    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from implementations_clean import *\n",
    "from proj1_helpers import *\n",
    "\n",
    "y,X,ids = load_csv_data(\"train.csv\")\n",
    "#ADD BIAS\n",
    "\n",
    "import pandas as pd\n",
    "X = np.where(X == -999., np.nan, X)\n",
    "df = pd.DataFrame(X)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    # converting -999. to nan to use np.nanmean and np.nanstd\n",
    "    X = np.where(X == -999., np.nan, X)\n",
    "    # standardizing the data Xd = (X_d - E[X_d])/(std(X_d))\n",
    "    X, means, stds = standardize(X)\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 1: correlations der_mass_MMC\n",
    "col_means = np.nanmean(X, axis=0)\n",
    "idxs = np.where(np.isnan(X))\n",
    "X[idxs] = np.take(col_means, idxs[1])\n",
    "X_gt_mmc = np.array(X[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X[:,0][X[:,0] > 140] = 140\n",
    "X = np.column_stack((X, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X[:,13]*np.cos(X[:,15])\n",
    "tau_py = X[:,13]*np.sin(X[:,15])\n",
    "tau_pz = X[:,13]*np.sinh(X[:,14])\n",
    "X = np.column_stack((X, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X[:,16]*np.cos(X[:,18])\n",
    "lep_py = X[:,16]*np.cos(X[:,18])\n",
    "lep_pz = X[:,16]*np.cos(X[:,17])\n",
    "X = np.column_stack((X, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X[:,22]*np.cos(X[:,24])\n",
    "jet_py = X[:,22]*np.cos(X[:,24])\n",
    "jet_pz = X[:,22]*np.cos(X[:,23])\n",
    "X = np.column_stack((X, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X[:,25]*np.cos(X[:,27])\n",
    "subjet_py = X[:,25]*np.cos(X[:,27])\n",
    "subjet_pz = X[:,25]*np.cos(X[:,26])\n",
    "X = np.column_stack((X, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "# feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X[:,11] = np.abs(X[:,11])\n",
    "#tau phi\n",
    "X[:,15] = np.abs(X[:,15])\n",
    "#lep phi\n",
    "X[:,18] = np.abs(X[:,18])\n",
    "#met phi\n",
    "X[:,20] = np.abs(X[:,20])\n",
    "#lead jet phi\n",
    "X[:,24] = np.abs(X[:,24])\n",
    "#sublead jet phi\n",
    "X[:,27] = np.abs(X[:,27])\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.head(20)\n",
    "X[:,11].mean()\n",
    "\n",
    "X = make_features(X)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# np.random.shuffle(X)\n",
    "cutoff = int(0.8*((X.shape)[0]))\n",
    "X_train = X[:cutoff]\n",
    "y_train = y[:cutoff]\n",
    "X_test = X[cutoff:]\n",
    "y_test = y[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.where(z < 0, 0, z)\n",
    "\n",
    "def relu_gradient(z):\n",
    "    return np.where(z < 0, 0, 1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def grad_loss(y_pred, y):\n",
    "    return y_pred - y\n",
    "\n",
    "def BCE_gradient(y,y_pred):\n",
    "    #return y_pred-y\n",
    "    return (-y/y_pred + (1-y)/(1-y_pred))\n",
    "\n",
    "class layer:\n",
    "\n",
    "    def __init__(self, dim_0, dim_1, activation):\n",
    "        self.w = np.random.randn(dim_0, dim_1) / np.sqrt(dim_0)\n",
    "        self.b = np.zeros(dim_0)\n",
    "        if activation == 'relu':\n",
    "            self.f = relu\n",
    "            self.f_grad = relu_gradient\n",
    "        if activation == 'sigmoid':\n",
    "            self.f = sigmoid\n",
    "            self.f_grad = sigmoid_gradient\n",
    "\n",
    "    def feed_forward(self, a):\n",
    "        self.a_prev = a\n",
    "        if len(a.shape) == 1:\n",
    "            # for batch_size 1\n",
    "            \n",
    "            self.z = np.dot(self.w, a) + self.b\n",
    "        else:\n",
    "            self.z = np.dot(self.w, a) + np.tile(self.b, (a.shape[1],1)).T\n",
    "        self.a = self.f(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def back_propagate(self, grad):\n",
    "        grad = grad * self.f_grad(self.z)\n",
    "        self.w_grad = grad @ self.a_prev.T\n",
    "        self.b_grad = np.sum(grad, axis = 1)\n",
    "        return (grad.T @ self.w).T\n",
    "\n",
    "\n",
    "class MLP_2:\n",
    "    # trying a vectorized MLP\n",
    "    def __init__(self, dim, activations):\n",
    "        \n",
    "        self.layers = []\n",
    "        for n in range(len(dim)-1):\n",
    "            self.layers.append(layer(dim[n + 1], dim[n], activations[n]))\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        a = X\n",
    "        for l in self.layers:\n",
    "            a = l.feed_forward(a)\n",
    "        return a\n",
    "    \n",
    "    def back_propagate(self, y_pred, y):\n",
    "        grad = BCE_gradient(y, y_pred)\n",
    "        for l in np.flip(self.layers):\n",
    "            grad = l.back_propagate(grad)\n",
    "        \n",
    "            \n",
    "    def gradient_descent_step(self, lambda_, weight_decay):\n",
    "        for l in self.layers:\n",
    "            l.w -= (l.w_grad + l.w * weight_decay) * lambda_\n",
    "            l.b -= (l.b_grad + l.b * weight_decay) * lambda_\n",
    "            \n",
    "    def train(self, X, Y, batch_size, max_iter, lambda_, weight_decay):\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            idxs = np.arange(X.shape[0])\n",
    "            np.random.shuffle(idxs)\n",
    "            idxs = idxs[:batch_size]\n",
    "            X_batch = X[idxs]\n",
    "            y_batch = y[idxs]\n",
    "            y_pred = self.feed_forward(X_batch.T)\n",
    "            self.back_propagate(y_pred, y_batch)\n",
    "            self.gradient_descent_step(lambda_ / batch_size, weight_decay)\n",
    "            if i % 500 == 0:\n",
    "                print(self.BCE_loss(X, Y))\n",
    "\n",
    "    def BCE_loss(self,X, y):\n",
    "        loss = 0\n",
    "        N = len(y)\n",
    "        for i in range(N):\n",
    "            y_pred = self.feed_forward(X[i])\n",
    "            eps = 1e-7\n",
    "            loss_i = -(y[i]*np.log(y_pred+eps) + (1-y[i])*np.log(1-y_pred+eps))\n",
    "            loss = loss + loss_i/N\n",
    "        return loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = X_train.shape[1]\n",
    "n_h1 = 70\n",
    "n_h2 = 70\n",
    "n_h3 = 70\n",
    "n_h4 = 70\n",
    "n_h5 = 70\n",
    "n_h6 = 70\n",
    "out_dim = 1\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,n_h4,n_h5,n_h6,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','relu','sigmoid']\n",
    "lambda_ = 0.001\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_2(dimensions, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68873247]\n",
      "[0.68237118]\n",
      "[0.68742344]\n",
      "[0.68040697]\n",
      "[0.67766475]\n",
      "[0.66861691]\n",
      "[0.66450832]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "max_iter = 1000\n",
    "lambda_ = 0.01\n",
    "weight_decay = 0.01\n",
    "\n",
    "mlp.train(X_train, y_train, batch_size, max_iter,  lambda_, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
