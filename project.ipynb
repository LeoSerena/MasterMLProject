{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from datetime import datetime\n",
    "np.random.seed(2)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X,ids = load_csv_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    # converting -999. to nan to use np.nanmean and np.nanstd\n",
    "    X = np.where(X == -999., np.nan, X)\n",
    "    # standardizing the data Xd = (X_d - E[X_d])/(std(X_d))\n",
    "    X, means, stds = standardize(X)\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_means = np.nanmean(X, axis=0)\n",
    "X = np.where(X == -999., np.nan, X)\n",
    "col_means = np.nanmedian(X, axis=0)\n",
    "idxs = np.where(np.isnan(X))\n",
    "X[idxs] = np.take(col_means, idxs[1])\n",
    "\n",
    "#feature 1: correlations der_mass_MMC\n",
    "X_gt_mmc = np.array(X[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X[:,0][X[:,0] > 140] = 140\n",
    "X = np.column_stack((X, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X[:,13]*np.cos(X[:,15])\n",
    "tau_py = X[:,13]*np.sin(X[:,15])\n",
    "tau_pz = X[:,13]*np.sinh(X[:,14])\n",
    "X = np.column_stack((X, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X[:,16]*np.cos(X[:,18])\n",
    "lep_py = X[:,16]*np.cos(X[:,18])\n",
    "lep_pz = X[:,16]*np.cos(X[:,17])\n",
    "X = np.column_stack((X, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X[:,23]*np.cos(X[:,25])\n",
    "jet_py = X[:,23]*np.cos(X[:,25])\n",
    "jet_pz = X[:,23]*np.cos(X[:,24])\n",
    "X = np.column_stack((X, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X[:,26]*np.cos(X[:,28])\n",
    "subjet_py = X[:,26]*np.cos(X[:,28])\n",
    "subjet_pz = X[:,26]*np.cos(X[:,27])\n",
    "X = np.column_stack((X, subjet_px,subjet_py,subjet_pz))\n",
    "#subleading jet momentum\n",
    "# DER_met_phi_centrality_cos = np.cos(X[:,11])\n",
    "# DER_met_phi_centrality_sin = np.sin(X[:,11])\n",
    "# X = np.column_stack((X, DER_met_phi_centrality_cos,DER_met_phi_centrality_sin))\n",
    "\n",
    "#feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X[:,11] = np.abs(X[:,11])\n",
    "#tau phi\n",
    "X[:,15] = np.abs(X[:,15])\n",
    "#lep phi\n",
    "X[:,18] = np.abs(X[:,18])\n",
    "#met phi\n",
    "X[:,20] = np.abs(X[:,20])\n",
    "#lead jet phi\n",
    "X[:,24] = np.abs(X[:,24])\n",
    "#sublead jet phi\n",
    "X[:,27] = np.abs(X[:,27])\n",
    "\n",
    "\n",
    "#feature 4: categorical PRI_jet_num\n",
    "jet_num_0 = (X[:,22] == 0).astype(int)\n",
    "jet_num_1 = (X[:,22] == 1).astype(int)\n",
    "jet_num_2 = (X[:,22] == 2).astype(int)\n",
    "jet_num_3 = (X[:,22] == 3).astype(int)\n",
    "\n",
    "# #feature 5: pt ratios\n",
    "# #tau_lep_ratio = PRI_tau_pt/PRI_lep_pt\n",
    "# tau_lep_ratio = X[:,13]/X[:,16]\n",
    "tau_lep_ratio = X[:,13]/X[:,16]\n",
    "# #jets_ratio = PRI_jet_leading_pt/PRI_jet_subleading_pt\n",
    "# jets_ratio = X[:,22]/X[:,25]\n",
    "# jets_ratio = X[:,23]/X[:,25]\n",
    "# #met_tot_ratio = PRI_met/PRI_met_sumet\n",
    "met_tot_ratio = X[:,19]/X[:,21]\n",
    "# X = np.column_stack((X, tau_lep_ratio,jets_ratio,met_tot_ratio))\n",
    "X = np.column_stack((X, tau_lep_ratio,met_tot_ratio))\n",
    "\n",
    "# #feature 6: jets_diff_angle\n",
    "jets_diff_angle = np.cos(X[:,24]-X[:,27])\n",
    "X = np.column_stack((X, jets_diff_angle))\n",
    "\n",
    "#TEST EXTRA COS/SIN ANGLES\n",
    "\n",
    "# df = pd.DataFrame(X)\n",
    "# df.head()\n",
    "# print(X[:,22] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares\n",
    "\n",
    "## gradient descent Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "losses = []\n",
    "ws = []\n",
    "gammas = np.linspace(0.01,0.03,21)\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_GD(y_train, X_train, np.zeros(X_train.shape[1]), max_iter, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.argmin(losses)\n",
    "w = ws[index]\n",
    "loss = losses[index]\n",
    "gammas[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 78.8415%, loss is of 0.15751242535210708\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 78.974 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_test - pred)) / X_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stochastic gradient descent least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "losses = []\n",
    "ws = []\n",
    "gammas = np.linspace(0.01,0.03,21)\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_SGD(y_train, X_train, np.zeros(X_train.shape[1]), max_iter, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.argmin(losses)\n",
    "w = ws[index]\n",
    "loss = losses[index]\n",
    "gammas[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 74.2975%, loss is of 0.20853730471403648\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 74.44800000000001 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_test - pred)) / X_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_(y, tx):\n",
    "    \"\"\"\n",
    "    finds least squares analytical solution\n",
    "    \n",
    "    input\n",
    "        y, the labels\n",
    "        tx, the training data\n",
    "    output\n",
    "        w, the learned weight vector\n",
    "        loss, the loss corresponding to the learned vector\n",
    "    \"\"\"\n",
    "    XT_X = tx.T @ tx\n",
    "    XT_Y = tx.T @ y\n",
    "    w = np.linalg.inv(XT_X) @ XT_Y\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-68a0ba96dc1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\ML_project\\implementations.py\u001b[0m in \u001b[0;36mleast_squares\u001b[1;34m(y, tx)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0mXT_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mXT_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXT_X\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mXT_Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 71.852 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_test - pred)) / X_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0.01, 0.1, 11)\n",
    "losses = []\n",
    "ws = []\n",
    "for lambda_ in lambdas:\n",
    "    w, loss = ridge_regression(X_train, y_train, lambda_)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for w in ws:\n",
    "    pred_test = X_test @ w\n",
    "    pred_test = np.where(pred_test > 1/2, 1, 0)\n",
    "    accuracy = 100 - 100 * np.sum(np.abs(y_test - pred_test)) / X_test.shape[0]\n",
    "    accuracies.append(accuracy)\n",
    "index = np.argmax(accuracies)\n",
    "w = ws[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 78.8605%, loss is of 0.16811917078285946\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 79.006 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_test - pred)) / X_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 2000\n",
    "gamma = 0.001\n",
    "w, loss = logistic_regression(y_train, X_train, np.zeros(X_train.shape[1]), max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 71.6995%, loss is of 0.5609871771634554\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reg logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leose\\Desktop\\ML_project\\implementations.py:172: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(y * np.log(pred(x,w)+eps) + (1-y) * np.log(1-pred(x,w)+eps))\n",
      "C:\\Users\\leose\\Desktop\\ML_project\\implementations.py:172: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.mean(y * np.log(pred(x,w)+eps) + (1-y) * np.log(1-pred(x,w)+eps))\n"
     ]
    }
   ],
   "source": [
    "max_iter = 1000\n",
    "gamma = 0.05\n",
    "lambda_ = 0.01\n",
    "w, loss = reg_logistic_regression(y_train, X_train, lambda_, np.zeros(X_train.shape[1]), max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 69.5365%, loss is of nan\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backprop\n",
    "\n",
    "For MSE:\n",
    "\n",
    "$ \n",
    "    \\frac{\\delta L}{a_n} = \\frac{\\delta (a_n - y)^2}{\\delta a_{n}} = 2(a_n - y)  \\\\\n",
    "    \\frac{\\delta a_{i}}{\\delta z_{i}} = \\frac{\\delta S(z_{i})}{\\delta z_{i}} = S(z_{i})(1 - S(z_{i})) \\\\ \n",
    "    \\frac{\\delta z_{i+1}}{\\delta w_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta w_{i}} = a_{i} \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta b_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta b_{i}} = 1  \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta a_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta a_{i}}= w_{i} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tryin' ma best to vectorize baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.where(z < 0, 0, z)\n",
    "\n",
    "def relu_gradient(z):\n",
    "    return np.where(z < 0, 0, 1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def grad_loss(y_pred, y):\n",
    "    return y_pred - y\n",
    "\n",
    "def BCE_gradient(y,y_pred):\n",
    "    #return y_pred-y\n",
    "    return (-y/y_pred + (1-y)/(1-y_pred))\n",
    "\n",
    "class layer:\n",
    "\n",
    "    def __init__(self, dim_0, dim_1, activation):\n",
    "        self.w = np.random.randn(dim_0, dim_1) / np.sqrt(dim_0)\n",
    "        self.b = np.zeros(dim_0)\n",
    "        if activation == 'relu':\n",
    "            self.f = relu\n",
    "            self.f_grad = relu_gradient\n",
    "        if activation == 'sigmoid':\n",
    "            self.f = sigmoid\n",
    "            self.f_grad = sigmoid_gradient\n",
    "\n",
    "    def feed_forward(self, a):\n",
    "        self.a_prev = a\n",
    "        if len(a.shape) == 1:\n",
    "            # for batch_size 1\n",
    "            self.z = np.dot(self.w, a) + self.b\n",
    "        else:\n",
    "            self.z = np.dot(self.w, a) + np.tile(self.b, (a.shape[1],1)).T\n",
    "        self.a = self.f(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def back_propagate(self, grad):\n",
    "        grad = grad * self.f_grad(self.z)\n",
    "        self.w_grad = grad @ self.a_prev.T\n",
    "        self.b_grad = np.sum(grad, axis = 1)\n",
    "        return (grad.T @ self.w).T\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    # trying a vectorized MLP\n",
    "    def __init__(self, dim, activations):\n",
    "        \n",
    "        self.layers = []\n",
    "        for n in range(len(dim)-1):\n",
    "            self.layers.append(layer(dim[n + 1], dim[n], activations[n]))\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        a = X\n",
    "        for l in self.layers:\n",
    "            a = l.feed_forward(a)\n",
    "        return a\n",
    "    \n",
    "    def back_propagate(self, y_pred, y):\n",
    "        grad = BCE_gradient(y, y_pred)\n",
    "        for l in np.flip(self.layers):\n",
    "            grad = l.back_propagate(grad)\n",
    "        \n",
    "            \n",
    "    def gradient_descent_step(self, gamma, weight_decay):\n",
    "        for l in self.layers:\n",
    "            l.w -= (l.w_grad + l.w * weight_decay) * gamma\n",
    "            l.b -= (l.b_grad + l.b * weight_decay) * gamma\n",
    "            \n",
    "    def train(self, X, Y, batch_size, max_iter, gamma, weight_decay, number_of_loss_computations = 5):\n",
    "        \n",
    "        start = datetime.now()\n",
    "        gamma = gamma\n",
    "        div = int(max_iter / number_of_loss_computations)\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            \n",
    "            if i == int(max_iter / 3):\n",
    "                gamma = 0.1 * gamma\n",
    "            if i == int(max_iter * 0.75):\n",
    "                gamma = 0.1 * gamma\n",
    "            if i % div == 0:\n",
    "                print(\"{}% of the way\".format(int(i/max_iter * 100)))\n",
    "                print(self.BCE_loss(X, Y))\n",
    "                \n",
    "            idxs = np.arange(X.shape[0])\n",
    "            np.random.shuffle(idxs)\n",
    "            idxs = idxs[:batch_size]\n",
    "            \n",
    "            X_batch = X[idxs]\n",
    "            y_batch = y[idxs]\n",
    "            y_pred = self.feed_forward(X_batch.T)\n",
    "            self.back_propagate(y_pred, y_batch)\n",
    "            self.gradient_descent_step(gamma / batch_size, weight_decay)\n",
    "\n",
    "        end = datetime.now()\n",
    "        print(\"time taken:\", end - start)\n",
    "                \n",
    "    def BCE_loss(self,X, y):\n",
    "        loss = 0\n",
    "        N = len(y)\n",
    "        for i in range(N):\n",
    "            y_pred = self.feed_forward(X[i])\n",
    "            eps = 1e-7\n",
    "            loss_i = -(y[i]*np.log(y_pred+eps) + (1-y[i])*np.log(1-y_pred+eps))\n",
    "            loss = loss + loss_i/N\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def BCE_loss_vect(self, X, y):\n",
    "        y_pred = self.feed_forward(X.T)\n",
    "        return np.mean(y_pred * np.log(y_pred + eps) + (1 - y_pred) * np.log(1 - y_pred + eps))\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = self.feed_forward(X.T)\n",
    "        return np.where(y < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "in_dim = X_train.shape[1]\n",
    "n_h1 = 100\n",
    "n_h2 = 100\n",
    "n_h3 = 100\n",
    "n_h4 = 100\n",
    "n_h5 = 100\n",
    "n_h6 = 100\n",
    "n_h7 = 100\n",
    "out_dim = 1\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,n_h4,n_h5,n_h6,n_h7,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','relu','relu','sigmoid']\n",
    "\n",
    "# mlp = MLP(gamma = gamma, dimensions = dimensions, activations = activations, weight_decay = weight_decay)\n",
    "\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,n_h4,n_h5,n_h6,n_h7,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','relu','relu','sigmoid']\n",
    "mlp_1 = MLP(dimensions, activations)\n",
    "dimensions = [in_dim, 30,30,30,out_dim]\n",
    "activations = ['relu','relu','relu','sigmoid']\n",
    "mlp_2 = MLP(dimensions, activations)\n",
    "dimensions = [in_dim, 50,50,50,50,50,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','sigmoid']\n",
    "mlp_3 = MLP(dimensions, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.001\n",
    "weight_decay = 0.001\n",
    "max_iter = 1750000\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of the way\n",
      "[0.71160321]\n",
      "20% of the way\n",
      "[0.36999201]\n",
      "40% of the way\n",
      "[0.3566317]\n",
      "60% of the way\n",
      "[0.35493575]\n",
      "80% of the way\n",
      "[0.35357132]\n",
      "time taken: 2:51:52.380364\n"
     ]
    }
   ],
   "source": [
    "mlp_1.train(X, y, batch_size, max_iter, gamma, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first MLP\n",
      "training accuracy: 77.5387073420389% | test accuracy: 77.57431404770195%\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = mlp_1.feed_forward(X_train.T)\n",
    "acc_train_1 = 1-np.sum(np.abs(y_pred1 - y_train)) / X_train.shape[0]\n",
    "y_pred1 = mlp_1.feed_forward(X_test.T)\n",
    "acc_test_1 = 1-np.sum(np.abs(y_pred1 - y_test)) / X_test.shape[0]\n",
    "print(\"first MLP\")\n",
    "print(\"training accuracy: {}% | test accuracy: {}%\".format(acc_train_1 * 100, acc_test_1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of the way\n",
      "[0.97566973]\n",
      "20% of the way\n",
      "[0.37495116]\n",
      "40% of the way\n",
      "[0.36193508]\n",
      "60% of the way\n",
      "[0.36064755]\n",
      "80% of the way\n",
      "[0.35980389]\n",
      "time taken: 2:46:53.610707\n"
     ]
    }
   ],
   "source": [
    "mlp_2.train(X, y, batch_size, max_iter, gamma, weight_decay * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second MLP\n",
      "training accuracy: 77.20870632413957% | test accuracy: 77.57431404770195%\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = mlp_2.feed_forward(X_train.T)\n",
    "acc_train_2 = 1-np.sum(np.abs(y_pred2 - y_train)) / X_train.shape[0]\n",
    "y_pred2 = mlp_1.feed_forward(X_test.T)\n",
    "acc_test_2 = 1-np.sum(np.abs(y_pred2 - y_test)) / X_test.shape[0]\n",
    "print(\"second MLP\")\n",
    "print(\"training accuracy: {}% | test accuracy: {}%\".format(acc_train_2 * 100, acc_test_2 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of the way\n",
      "[0.76225907]\n",
      "20% of the way\n",
      "[0.37208863]\n",
      "40% of the way\n",
      "[0.35816828]\n",
      "60% of the way\n",
      "[0.3567275]\n",
      "80% of the way\n",
      "[0.35590344]\n",
      "time taken: 2:54:53.654701\n"
     ]
    }
   ],
   "source": [
    "mlp_3.train(X, y, batch_size, max_iter, gamma, weight_decay * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "third MLP\n",
      "training accuracy: 77.53445947453166% | test accuracy: 77.57396327955978%\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = mlp_3.feed_forward(X_train.T)\n",
    "acc_train_3 = 1-np.sum(np.abs(y_pred3 - y_train)) / X_train.shape[0]\n",
    "y_pred3 = mlp_3.feed_forward(X_test.T)\n",
    "acc_test_3 = 1-np.sum(np.abs(y_pred3 - y_test)) / X_test.shape[0]\n",
    "print(\"third MLP\")\n",
    "print(\"training accuracy: {}% | test accuracy: {}%\".format(acc_train_3 * 100, acc_test_3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,X_sub,ids = load_csv_data(\"test.csv\")\n",
    "#feature 1: correlations der_mass_MMC\n",
    "X_sub = np.where(X_sub == -999., np.nan, X_sub)\n",
    "col_means = np.nanmean(X_sub, axis=0)\n",
    "idxs = np.where(np.isnan(X_sub))\n",
    "X_sub[idxs] = np.take(col_means, idxs[1])\n",
    "X_gt_mmc = np.array(X_sub[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X_sub[:,0][X_sub[:,0] > 140] = 140\n",
    "X_sub = np.column_stack((X_sub, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X_sub[:,13]*np.cos(X_sub[:,15])\n",
    "tau_py = X_sub[:,13]*np.sin(X_sub[:,15])\n",
    "tau_pz = X_sub[:,13]*np.sinh(X_sub[:,14])\n",
    "X_sub = np.column_stack((X_sub, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X_sub[:,16]*np.cos(X_sub[:,18])\n",
    "lep_py = X_sub[:,16]*np.cos(X_sub[:,18])\n",
    "lep_pz = X_sub[:,16]*np.cos(X_sub[:,17])\n",
    "X_sub = np.column_stack((X_sub, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X_sub[:,22]*np.cos(X_sub[:,24])\n",
    "jet_py = X_sub[:,22]*np.cos(X_sub[:,24])\n",
    "jet_pz = X_sub[:,22]*np.cos(X_sub[:,23])\n",
    "X_sub = np.column_stack((X_sub, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X_sub[:,25]*np.cos(X_sub[:,27])\n",
    "subjet_py = X_sub[:,25]*np.cos(X_sub[:,27])\n",
    "subjet_pz = X_sub[:,25]*np.cos(X_sub[:,26])\n",
    "X_sub = np.column_stack((X_sub, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "# feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X_sub[:,11] = np.abs(X_sub[:,11])\n",
    "#tau phi\n",
    "X_sub[:,15] = np.abs(X_sub[:,15])\n",
    "#lep phi\n",
    "X_sub[:,18] = np.abs(X_sub[:,18])\n",
    "#met phi\n",
    "X_sub[:,20] = np.abs(X_sub[:,20])\n",
    "#lead jet phi\n",
    "X_sub[:,24] = np.abs(X_sub[:,24])\n",
    "#sublead jet phi\n",
    "X_sub[:,27] = np.abs(X_sub[:,27])\n",
    "\n",
    "X_sub = make_features(X_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_1 = mlp_1.feed_forward(X_sub.T)\n",
    "p_2 = mlp_2.feed_forward(X_sub.T)\n",
    "p_3 = mlp_3.feed_forward(X_sub.T)\n",
    "p = np.mean((p_1,p_2,p_3),axis = 0)\n",
    "p = p > 0.5\n",
    "sub_pred = p*2 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = sub_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, sub_pred, \"submission_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv(\"submission_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv(\"submission_test.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
