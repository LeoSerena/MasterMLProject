{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from datetime import datetime\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X,ids = load_csv_data(\"train.csv\")\n",
    "#ADD BIAS\n",
    "import pandas as pd\n",
    "X = np.where(X == -999., np.nan, X)\n",
    "df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    # converting -999. to nan to use np.nanmean and np.nanstd\n",
    "    X = np.where(X == -999., np.nan, X)\n",
    "    # standardizing the data Xd = (X_d - E[X_d])/(std(X_d))\n",
    "    X, means, stds = standardize(X)\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 1: correlations der_mass_MMC\n",
    "col_means = np.nanmean(X, axis=0)\n",
    "idxs = np.where(np.isnan(X))\n",
    "X[idxs] = np.take(col_means, idxs[1])\n",
    "X_gt_mmc = np.array(X[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X[:,0][X[:,0] > 140] = 140\n",
    "X = np.column_stack((X, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X[:,13]*np.cos(X[:,15])\n",
    "tau_py = X[:,13]*np.sin(X[:,15])\n",
    "tau_pz = X[:,13]*np.sinh(X[:,14])\n",
    "X = np.column_stack((X, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X[:,16]*np.cos(X[:,18])\n",
    "lep_py = X[:,16]*np.cos(X[:,18])\n",
    "lep_pz = X[:,16]*np.cos(X[:,17])\n",
    "X = np.column_stack((X, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X[:,22]*np.cos(X[:,24])\n",
    "jet_py = X[:,22]*np.cos(X[:,24])\n",
    "jet_pz = X[:,22]*np.cos(X[:,23])\n",
    "X = np.column_stack((X, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X[:,25]*np.cos(X[:,27])\n",
    "subjet_py = X[:,25]*np.cos(X[:,27])\n",
    "subjet_pz = X[:,25]*np.cos(X[:,26])\n",
    "X = np.column_stack((X, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "# feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X[:,11] = np.abs(X[:,11])\n",
    "#tau phi\n",
    "X[:,15] = np.abs(X[:,15])\n",
    "#lep phi\n",
    "X[:,18] = np.abs(X[:,18])\n",
    "#met phi\n",
    "X[:,20] = np.abs(X[:,20])\n",
    "#lead jet phi\n",
    "X[:,24] = np.abs(X[:,24])\n",
    "#sublead jet phi\n",
    "X[:,27] = np.abs(X[:,27])\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.head(20)\n",
    "X[:,11].mean()\n",
    "\n",
    "X = make_features(X)\n",
    "\n",
    "cutoff = int(0.8*((X.shape)[0]))\n",
    "X_train = X[:cutoff]\n",
    "y_train = y[:cutoff]\n",
    "X_test = X[cutoff:]\n",
    "y_test = y[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares\n",
    "\n",
    "## gradient descent Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 50\n",
    "losses = []\n",
    "ws = []\n",
    "gammas = np.linspace(0.1,0.3,21)\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_GD(y_train, X_train, np.zeros(X_train.shape[1]), max_iter, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.argmin(losses)\n",
    "w = ws[index]\n",
    "loss = losses[index]\n",
    "gammas[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 79.01599999999999%, loss is of 0.15698247190125425\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 79.242 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_test - pred)) / X_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stochastic gradient descent least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "losses = []\n",
    "ws = []\n",
    "gammas = np.linspace(0.01,0.03,21)\n",
    "for gamma in gammas:\n",
    "    w, loss = least_squares_SGD(y_train, X_train, np.zeros(X_train.shape[1]), max_iter, gamma)\n",
    "    losses.append(loss)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.argmin(losses)\n",
    "w = ws[index]\n",
    "loss = losses[index]\n",
    "gammas[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 73.3605%, loss is of 0.2126655179650995\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 73.394 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_test - pred)) / X_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = least_squares(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 69.9195%, loss is of 0.2785096416848562\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 70.248 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_test - pred)) / X_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0.01, 0.1, 11)\n",
    "losses = []\n",
    "ws = []\n",
    "for lambda_ in lambdas:\n",
    "    w, loss = ridge_regression(X_train, y_train, lambda_)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for w in ws:\n",
    "    pred_test = X_test @ w\n",
    "    pred_test = np.where(pred_test > 1/2, 1, 0)\n",
    "    accuracy = 100 - 100 * np.sum(np.abs(y_test - pred_test)) / X_test.shape[0]\n",
    "    accuracies.append(accuracy)\n",
    "index = np.argmax(accuracies)\n",
    "w = ws[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 78.998%, loss is of 0.16702375764989005\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is of 79.166 %\n"
     ]
    }
   ],
   "source": [
    "pred = X_test @ w\n",
    "pred = np.where(pred > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_test - pred)) / X_test.shape[0]\n",
    "print(\"accuracy on test set is of {} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "gamma = 0.05\n",
    "w, loss = logistic_regression(y_train, X_train, np.zeros(X.shape[1]), max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 69.9105%, loss is of 2.7688947890856888\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reg logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "gamma = 0.05\n",
    "lambda_ = 0.01\n",
    "w, loss = reg_logistic_regression(y_train, X_train, lambda_, np.zeros(X.shape[1]), max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set is of 75.7025%, loss is of 0.4936587877193814\n"
     ]
    }
   ],
   "source": [
    "pred_tr = X_train @ w\n",
    "pred_tr = np.where(pred_tr > 1/2, 1, 0)\n",
    "accuracy = 100 - 100 * np.sum(np.abs(y_train - pred_tr)) / X_train.shape[0]\n",
    "print(\"accuracy on training set is of {}%, loss is of {}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backprop\n",
    "\n",
    "For MSE:\n",
    "\n",
    "$ \n",
    "    \\frac{\\delta L}{a_n} = \\frac{\\delta (a_n - y)^2}{\\delta a_{n}} = 2(a_n - y)  \\\\\n",
    "    \\frac{\\delta a_{i}}{\\delta z_{i}} = \\frac{\\delta S(z_{i})}{\\delta z_{i}} = S(z_{i})(1 - S(z_{i})) \\\\ \n",
    "    \\frac{\\delta z_{i+1}}{\\delta w_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta w_{i}} = a_{i} \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta b_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta b_{i}} = 1  \\\\\n",
    "    \\frac{\\delta z_{i+1}}{\\delta a_{i}} = \\frac{\\delta (a_{i} * w_{i} + b_{i})}{\\delta a_{i}}= w_{i} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tryin' ma best to vectorize baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.where(z < 0, 0, z)\n",
    "\n",
    "def relu_gradient(z):\n",
    "    return np.where(z < 0, 0, 1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def grad_loss(y_pred, y):\n",
    "    return y_pred - y\n",
    "\n",
    "def BCE_gradient(y,y_pred):\n",
    "    #return y_pred-y\n",
    "    return (-y/y_pred + (1-y)/(1-y_pred))\n",
    "\n",
    "class layer:\n",
    "\n",
    "    def __init__(self, dim_0, dim_1, activation):\n",
    "        self.w = np.random.randn(dim_0, dim_1) / np.sqrt(dim_0)\n",
    "        self.b = np.zeros(dim_0)\n",
    "        if activation == 'relu':\n",
    "            self.f = relu\n",
    "            self.f_grad = relu_gradient\n",
    "        if activation == 'sigmoid':\n",
    "            self.f = sigmoid\n",
    "            self.f_grad = sigmoid_gradient\n",
    "\n",
    "    def feed_forward(self, a):\n",
    "        self.a_prev = a\n",
    "        if len(a.shape) == 1:\n",
    "            # for batch_size 1\n",
    "            self.z = np.dot(self.w, a) + self.b\n",
    "        else:\n",
    "            self.z = np.dot(self.w, a) + np.tile(self.b, (a.shape[1],1)).T\n",
    "        self.a = self.f(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def back_propagate(self, grad):\n",
    "        grad = grad * self.f_grad(self.z)\n",
    "        self.w_grad = grad @ self.a_prev.T\n",
    "        self.b_grad = np.sum(grad, axis = 1)\n",
    "        return (grad.T @ self.w).T\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    # trying a vectorized MLP\n",
    "    def __init__(self, dim, activations):\n",
    "        \n",
    "        self.layers = []\n",
    "        for n in range(len(dim)-1):\n",
    "            self.layers.append(layer(dim[n + 1], dim[n], activations[n]))\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        a = X\n",
    "        for l in self.layers:\n",
    "            a = l.feed_forward(a)\n",
    "        return a\n",
    "    \n",
    "    def back_propagate(self, y_pred, y):\n",
    "        grad = BCE_gradient(y, y_pred)\n",
    "        for l in np.flip(self.layers):\n",
    "            grad = l.back_propagate(grad)\n",
    "        \n",
    "            \n",
    "    def gradient_descent_step(self, gamma, weight_decay):\n",
    "        for l in self.layers:\n",
    "            l.w -= (l.w_grad + l.w * weight_decay) * gamma\n",
    "            l.b -= (l.b_grad + l.b * weight_decay) * gamma\n",
    "            \n",
    "    def train(self, X, Y, batch_size, max_iter, gamma, weight_decay, number_of_loss_computations = 5):\n",
    "        \n",
    "        start = datetime.now()\n",
    "        gamma = gamma\n",
    "        div = int(max_iter / number_of_loss_computations)\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            \n",
    "            if i == int(max_iter / 3):\n",
    "                gamma = 0.1 * gamma\n",
    "            if i % div == 0:\n",
    "                print(\"{}% of the way\".format(int(i/max_iter * 100)))\n",
    "                print(self.BCE_loss(X, Y))\n",
    "                \n",
    "            idxs = np.arange(X.shape[0])\n",
    "            np.random.shuffle(idxs)\n",
    "            idxs = idxs[:batch_size]\n",
    "            \n",
    "            X_batch = X[idxs]\n",
    "            y_batch = y[idxs]\n",
    "            y_pred = self.feed_forward(X_batch.T)\n",
    "            self.back_propagate(y_pred, y_batch)\n",
    "            self.gradient_descent_step(gamma / batch_size, weight_decay)\n",
    "\n",
    "        end = datetime.now()\n",
    "        print(\"time taken:\", end - start)\n",
    "                \n",
    "    def BCE_loss(self,X, y):\n",
    "        loss = 0\n",
    "        N = len(y)\n",
    "        for i in range(N):\n",
    "            y_pred = self.feed_forward(X[i])\n",
    "            eps = 1e-7\n",
    "            loss_i = -(y[i]*np.log(y_pred+eps) + (1-y[i])*np.log(1-y_pred+eps))\n",
    "            loss = loss + loss_i/N\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def BCE_loss_vect(self, X, y):\n",
    "        y_pred = self.feed_forward(X.T)\n",
    "        return np.mean(y_pred * np.log(y_pred + eps) + (1 - y_pred) * np.log(1 - y_pred + eps))\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = self.feed_forward(X.T)\n",
    "        return np.where(y < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "in_dim = X_train.shape[1]\n",
    "n_h1 = 100\n",
    "n_h2 = 100\n",
    "n_h3 = 100\n",
    "n_h4 = 100\n",
    "n_h5 = 100\n",
    "n_h6 = 100\n",
    "n_h7 = 100\n",
    "out_dim = 1\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,n_h4,n_h5,n_h6,n_h7,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','relu','relu','sigmoid']\n",
    "\n",
    "# mlp = MLP(gamma = gamma, dimensions = dimensions, activations = activations, weight_decay = weight_decay)\n",
    "\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,n_h4,n_h5,n_h6,n_h7,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','relu','relu','sigmoid']\n",
    "mlp_1 = MLP(dimensions, activations)\n",
    "dimensions = [in_dim, 30,30,30,out_dim]\n",
    "activations = ['relu','relu','relu','sigmoid']\n",
    "mlp_2 = MLP(dimensions, activations)\n",
    "dimensions = [in_dim, 50,50,50,50,50,out_dim]\n",
    "activations = ['relu','relu','relu','relu','relu','sigmoid']\n",
    "mlp_3 = MLP(dimensions, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.0001\n",
    "weight_decay = 0.001\n",
    "max_iter = 50000\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1.train(X, y, batch_size, max_iter, gamma, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = mlp_1.feed_forward(X_train.T)\n",
    "acc_train_1 = 1-np.sum(np.abs(y_pred1 - y_train)) / X_train.shape[0]\n",
    "y_pred1 = mlp_1.feed_forward(X_test.T)\n",
    "acc_test_1 = 1-np.sum(np.abs(y_pred1 - y_test)) / X_test.shape[0]\n",
    "print(\"first MLP\")\n",
    "print(\"training accuracy: {}% | test accuracy: {}%\".format(acc_train_1 * 100, acc_test_1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2.train(X, y, batch_size, max_iter, gamma, weight_decay * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = mlp_2.feed_forward(X_train.T)\n",
    "acc_train_2 = 1-np.sum(np.abs(y_pred2 - y_train)) / X_train.shape[0]\n",
    "y_pred2 = mlp_1.feed_forward(X_test.T)\n",
    "acc_test_2 = 1-np.sum(np.abs(y_pred2 - y_test)) / X_test.shape[0]\n",
    "print(\"second MLP\")\n",
    "print(\"training accuracy: {}% | test accuracy: {}%\".format(acc_train_2 * 100, acc_test_2 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3.train(X, y, batch_size, max_iter, gamma, weight_decay * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = mlp_3.feed_forward(X_train.T)\n",
    "acc_train_3 = 1-np.sum(np.abs(y_pred3 - y_train)) / X_train.shape[0]\n",
    "y_pred3 = mlp_3.feed_forward(X_test.T)\n",
    "acc_test_3 = 1-np.sum(np.abs(y_pred3 - y_test)) / X_test.shape[0]\n",
    "print(\"third MLP\")\n",
    "print(\"training accuracy: {}% | test accuracy: {}%\".format(acc_train_3 * 100, acc_test_3 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,X_sub,ids = load_csv_data(\"test.csv\")\n",
    "#feature 1: correlations der_mass_MMC\n",
    "X_sub = np.where(X_sub == -999., np.nan, X_sub)\n",
    "col_means = np.nanmean(X_sub, axis=0)\n",
    "idxs = np.where(np.isnan(X_sub))\n",
    "X_sub[idxs] = np.take(col_means, idxs[1])\n",
    "X_gt_mmc = np.array(X_sub[:,0], copy=True)\n",
    "X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "# X = np.column_stack((X, X_gt_mmc))\n",
    "X_sub[:,0][X_sub[:,0] > 140] = 140\n",
    "X_sub = np.column_stack((X_sub, X_gt_mmc))\n",
    "\n",
    "#feature 2: add momentums\n",
    "#tau momentum\n",
    "tau_px = X_sub[:,13]*np.cos(X_sub[:,15])\n",
    "tau_py = X_sub[:,13]*np.sin(X_sub[:,15])\n",
    "tau_pz = X_sub[:,13]*np.sinh(X_sub[:,14])\n",
    "X_sub = np.column_stack((X_sub, tau_px,tau_py,tau_pz))\n",
    "#lep momentum\n",
    "lep_px = X_sub[:,16]*np.cos(X_sub[:,18])\n",
    "lep_py = X_sub[:,16]*np.cos(X_sub[:,18])\n",
    "lep_pz = X_sub[:,16]*np.cos(X_sub[:,17])\n",
    "X_sub = np.column_stack((X_sub, lep_px,lep_py,lep_pz))\n",
    "#leading jet momentum\n",
    "jet_px = X_sub[:,22]*np.cos(X_sub[:,24])\n",
    "jet_py = X_sub[:,22]*np.cos(X_sub[:,24])\n",
    "jet_pz = X_sub[:,22]*np.cos(X_sub[:,23])\n",
    "X_sub = np.column_stack((X_sub, jet_px,jet_py,jet_pz))\n",
    "#subleading jet momentum\n",
    "subjet_px = X_sub[:,25]*np.cos(X_sub[:,27])\n",
    "subjet_py = X_sub[:,25]*np.cos(X_sub[:,27])\n",
    "subjet_pz = X_sub[:,25]*np.cos(X_sub[:,26])\n",
    "X_sub = np.column_stack((X_sub, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "# feature 3: abs angles\n",
    "#der_met_phi_centrality\n",
    "X_sub[:,11] = np.abs(X_sub[:,11])\n",
    "#tau phi\n",
    "X_sub[:,15] = np.abs(X_sub[:,15])\n",
    "#lep phi\n",
    "X_sub[:,18] = np.abs(X_sub[:,18])\n",
    "#met phi\n",
    "X_sub[:,20] = np.abs(X_sub[:,20])\n",
    "#lead jet phi\n",
    "X_sub[:,24] = np.abs(X_sub[:,24])\n",
    "#sublead jet phi\n",
    "X_sub[:,27] = np.abs(X_sub[:,27])\n",
    "\n",
    "X_sub = make_features(X_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_1 = mlp_1.feed_forward(X_sub.T)\n",
    "p_2 = mlp_2.feed_forward(X_sub.T)\n",
    "p_3 = mlp_3.feed_forward(X_sub.T)\n",
    "p = np.mean((p_1,p_2,p_3),axis = 0)\n",
    "p = p > 0.5\n",
    "sub_pred = p*2 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = sub_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, sub_pred, \"submission_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv(\"submission_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv(\"submission_test.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
