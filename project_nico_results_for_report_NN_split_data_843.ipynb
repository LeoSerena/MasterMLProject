{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations_clean import *\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y,X,ids = load_csv_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.where(X == -999., np.nan, X)\n",
    "y_1 = y[~np.isnan(X).any(axis=1)]\n",
    "X_1 = X[~np.isnan(X).any(axis=1)]\n",
    "y_2 = y[np.isnan(X).any(axis=1)]\n",
    "X_2 = X[np.isnan(X).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poly_features(X):\n",
    "    X_copy = np.copy(X)\n",
    "    for i in range(X_copy.shape[1]):\n",
    "        for j in range(i,X_copy.shape[1]):\n",
    "            X = np.column_stack((X, X_copy[:,i]*X_copy[:,j]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    # converting -999. to nan to use np.nanmean and np.nanstd\n",
    "    X = np.where(X == -999., np.nan, X)\n",
    "    # standardizing the data Xd = (X_d - E[X_d])/(std(X_d))\n",
    "    X, means, stds = standardize(X)\n",
    "    # since data is standirdized, the mean is more or less 0 for each feature so replacing by zero is reasonable and helps computations\n",
    "    X = np.where(np.isnan(X), 0, X)\n",
    "    # adding the 1 padding\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc(X):\n",
    "    col_means = np.nanmean(X, axis=0)\n",
    "    col_means = np.nanmedian(X, axis=0)\n",
    "    idxs = np.where(np.isnan(X))\n",
    "    X[idxs] = np.take(col_means, idxs[1])\n",
    "\n",
    "    #feature 1: correlations der_mass_MMC\n",
    "    X_gt_mmc = np.array(X[:,0], copy=True)\n",
    "    # X_0_cop = np.array(X[:,0], copy=True)\n",
    "    X_gt_mmc[X_gt_mmc <= 140] = 140\n",
    "    # X = np.column_stack((X, X_gt_mmc))\n",
    "    X[:,0][X[:,0] > 140] = 140\n",
    "    X = np.column_stack((X, X_gt_mmc))\n",
    "\n",
    "    #feature 2: add momentums\n",
    "    #tau momentum\n",
    "    tau_px = X[:,13]*np.cos(X[:,15])\n",
    "    tau_py = X[:,13]*np.sin(X[:,15])\n",
    "    tau_pz = X[:,13]*np.sinh(X[:,14])\n",
    "    tau_mod = X[:,13]*np.cosh(X[:,14])\n",
    "    X = np.column_stack((X, tau_px,tau_py,tau_pz))\n",
    "    #lep momentum\n",
    "    lep_px = X[:,16]*np.cos(X[:,18])\n",
    "    lep_py = X[:,16]*np.sin(X[:,18])\n",
    "    lep_pz = X[:,16]*np.sinh(X[:,17])\n",
    "    lep_mod = X[:,16]*np.cosh(X[:,17])\n",
    "    X = np.column_stack((X, lep_px,lep_py,lep_pz))\n",
    "    #leading jet momentum\n",
    "    jet_px = X[:,23]*np.cos(X[:,25])\n",
    "    jet_py = X[:,23]*np.sin(X[:,25])\n",
    "    jet_pz = X[:,23]*np.sinh(X[:,24])\n",
    "    jet_mod = X[:,23]*np.cosh(X[:,24])\n",
    "    X = np.column_stack((X, jet_px,jet_py,jet_pz))\n",
    "    #subleading jet momentum\n",
    "    subjet_px = X[:,26]*np.cos(X[:,28])\n",
    "    subjet_py = X[:,26]*np.sin(X[:,28])\n",
    "    subjet_pz = X[:,26]*np.sinh(X[:,27])\n",
    "    subjet_mod = X[:,26]*np.cosh(X[:,27])\n",
    "    X = np.column_stack((X, subjet_px,subjet_py,subjet_pz))\n",
    "\n",
    "    #feature 8: total invariant mass\n",
    "    term_1 = np.sqrt(tau_px**2 + tau_py**2 + tau_pz**2) + np.sqrt(lep_px**2 + lep_py**2 + lep_pz**2) \\\n",
    "    + np.sqrt(jet_px**2 + jet_py**2 + jet_pz**2) + np.sqrt(subjet_px**2 + subjet_py**2 + subjet_pz**2)\n",
    "    term_2 = (tau_px + lep_px + jet_px + subjet_px)**2 + (tau_py + lep_py + jet_py + subjet_py)**2 \\\n",
    "            + (tau_pz + lep_pz + jet_pz + subjet_pz)**2\n",
    "    inv_mass = np.sqrt(term_1**2 - term_2)\n",
    "\n",
    "\n",
    "    #feature 3: abs angles\n",
    "    #der_met_phi_centrality\n",
    "    X[:,11] = np.abs(X[:,11])\n",
    "    #tau phi\n",
    "    X[:,15] = np.abs(X[:,15])\n",
    "    #lep phi\n",
    "    X[:,18] = np.abs(X[:,18])\n",
    "    #met phi\n",
    "    X[:,20] = np.abs(X[:,20])\n",
    "    #lead jet phi\n",
    "    X[:,24] = np.abs(X[:,24])\n",
    "    #sublead jet phi\n",
    "    X[:,27] = np.abs(X[:,27])\n",
    "    #R sep abs\n",
    "    X[:,7] = np.abs(X[:,7])\n",
    "\n",
    "    #feature 9: log\n",
    "    inv_log_cols = (0,1,2,3,4,5,7,8,9,10,12,13,16,19,21,23,26)\n",
    "    X_inv_log_cols = np.log(1 / (1 + X[:, inv_log_cols]))\n",
    "    X = np.hstack((X, X_inv_log_cols))\n",
    "    # X_test_inv_log_cols = np.log(1 / (1 + X_test[:, inv_log_cols]))\n",
    "    # X_test = np.hstack((X_test, X_test_inv_log_cols))\n",
    "\n",
    "\n",
    "    #feature 4: categorical PRI_jet_num\n",
    "    jet_num_0 = (X[:,22] == 0).astype(int)\n",
    "    jet_num_1 = (X[:,22] == 1).astype(int)\n",
    "    jet_num_2 = (X[:,22] == 2).astype(int)\n",
    "    jet_num_3 = (X[:,22] == 3).astype(int)\n",
    "\n",
    "    # #feature 5: pt ratios\n",
    "    # #tau_lep_ratio = PRI_tau_pt/PRI_lep_pt\n",
    "    tau_lep_ratio = X[:,13]/X[:,16]\n",
    "    # #met_tot_ratio = PRI_met/PRI_met_sumet\n",
    "    met_tot_ratio = X[:,19]/X[:,21]\n",
    "    # X = np.column_stack((X, tau_lep_ratio,jets_ratio,met_tot_ratio))\n",
    "    X = np.column_stack((X, tau_lep_ratio,met_tot_ratio))\n",
    "\n",
    "    # #feature 6: jets_diff_angle\n",
    "    jets_diff_angle = np.cos(X[:,24]-X[:,27])\n",
    "    X = np.column_stack((X, jets_diff_angle))\n",
    "\n",
    "#     X = np.column_stack((X, inv_mass))\n",
    "    X = make_features(X)\n",
    "    X = np.column_stack((X, jet_num_0, jet_num_1, jet_num_2, jet_num_3))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_1 = preproc(X_1)\n",
    "X_2 = preproc(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.random.shuffle(X)\n",
    "cutoff_1 = int(0.8*((X_1.shape)[0]))\n",
    "cutoff_2 = int(0.8*((X_2.shape)[0]))\n",
    "X_1_train = X_1#[:cutoff_1]\n",
    "y_1_train = y_1#[:cutoff_1]\n",
    "X_1_test = X_1[cutoff_1:]\n",
    "y_1_test = y_1[cutoff_1:]\n",
    "X_2_train = X_2#[:cutoff_2]\n",
    "y_2_train = y_2#[:cutoff_2]\n",
    "X_2_test = X_2[cutoff_2:]\n",
    "y_2_test = y_2[cutoff_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP:    \n",
    "    #activations: 'relu', 'sigmoid', 'linear'\n",
    "    #loss assumed to be BCE\n",
    "    def __init__(self, gamma = 0.001,  dimensions = [2,10,1], activations = ['relu','sigmoid'] ,weight_decay = 0):\n",
    "        assert (len(dimensions)-1) == len(activations), \"Number of dimensions and activation functions do not match\"\n",
    "        # number of layers of our MLP\n",
    "        self.num_layers = len(dimensions)\n",
    "        self.gamma = gamma\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        # the first layer is the input data\n",
    "        self.activations = {}\n",
    "        self.activations_grad = {}\n",
    "        \n",
    "        for n in np.arange(self.num_layers - 1):\n",
    "            # the weights are initialized acccording to a normal distribution and divided by the size of the layer they're on\n",
    "            self.weights[n + 1] = np.random.randn(dimensions[n + 1],dimensions[n]) / np.sqrt(dimensions[n])\n",
    "            # bias are all initialized to zero\n",
    "            self.bias[n + 1] = np.zeros(dimensions[n + 1])\n",
    "            \n",
    "            if activations[n] == 'relu':\n",
    "                self.activations[n+1] = self.relu\n",
    "                self.activations_grad[n+1] = self.relu_gradient\n",
    "            elif activations[n] == 'sigmoid':\n",
    "                self.activations[n+1] = self.sigmoid\n",
    "                self.activations_grad[n+1] = self.sigmoid_gradient\n",
    "            else:\n",
    "                self.activations[n+1] = lambda x : x\n",
    "                self.activations_grad[n+1] = lambda x : 1\n",
    "    \n",
    "    def feed_forward(self, x):        \n",
    "        # keep track of all z and a to compute gradient in the backpropagation\n",
    "        z = {}\n",
    "        # the first layer is the input data\n",
    "        a = {1:x}\n",
    "        # We compute z[n+1] = a[n] * w[n] + b[n]\n",
    "        # and a[n+1] = f(z[n+1]) = f(a[n] * x[n] + b[n]) where * is the inner product\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            z[n + 1] = self.weights[n] @ a[n] + self.bias[n]\n",
    "            a[n + 1] = self.activations[n](z[n + 1])\n",
    "        y_pred = a[n+1]    \n",
    "        return y_pred,a, z\n",
    "    \n",
    "    # returns a prediction\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y_i_proba,_,_ = self.feed_forward(X[i].squeeze()) \n",
    "            preds[i] = (y_i_proba > 0.5)\n",
    "        return preds\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y_i_proba,_,_ = self.feed_forward(X[i].squeeze()) \n",
    "            preds[i] = y_i_proba\n",
    "        return preds\n",
    "    \n",
    "    def back_propagate(self, y,y_pred, a, z):\n",
    "        \n",
    "        weights_gradient = {}\n",
    "        bias_gradient = {}\n",
    "        \n",
    "        nabla = self.BCE_gradient(y,y_pred)\n",
    "        \n",
    "        for n in np.flip(np.arange(1, self.num_layers)):\n",
    "            nabla = nabla * self.activations_grad[n](z[n+1])\n",
    "            weights_gradient[n] = np.outer(nabla, a[n])\n",
    "            bias_gradient[n] = nabla\n",
    "            nabla = nabla @ self.weights[n]\n",
    "        \n",
    "        return weights_gradient, bias_gradient\n",
    "        ## self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "    \n",
    "    #weight decay : l2 reg\n",
    "    def gradient_descent_step(self, weights_gradient, bias_gradient):\n",
    "        for n in np.arange(1, self.num_layers):\n",
    "            self.weights[n] = self.weights[n] - self.gamma * (weights_gradient[n] + self.weight_decay*self.weights[n])\n",
    "            self.bias[n] = self.bias[n] - self.gamma * (bias_gradient[n] + self.weight_decay*self.bias[n])            \n",
    "    \n",
    "    #batch size = 1 for now\n",
    "    def train(self, X, y, max_iter, batch_size = 1, decay = False, decay_rate = 3, decay_iteration = 0):\n",
    "        for i in range(max_iter):\n",
    "            if (decay):\n",
    "                if ((i % decay_iteration == 0) and (i != 0)):\n",
    "                    print(\"Iteration: {}\".format(i))\n",
    "                    print(\"Decay, lr : {}\".format(self.gamma))\n",
    "                    self.gamma = self.gamma/decay_rate\n",
    "                    print(\"Decay, lr : {}\".format(self.gamma))\n",
    "                    print(\"\")\n",
    "            idxs = np.random.randint(0, X.shape[0],batch_size)\n",
    "            X_batch = X[idxs].squeeze()\n",
    "            y_batch = y[idxs]\n",
    "            y_pred,a, z = self.feed_forward(X_batch)\n",
    "            weights_gradient, bias_gradient = self.back_propagate(y_batch,y_pred,a, z)\n",
    "            self.gradient_descent_step(weights_gradient, bias_gradient)\n",
    "            if ((i % int(max_iter/5)) == 0):\n",
    "                loss = self.BCE_loss(X,y)\n",
    "                print(\"Iteration : {}, loss : {}\".format(i,loss))\n",
    "        loss = self.BCE_loss(X,y)\n",
    "        print(\"Iteration : {}, loss : {}\".format(i,loss))\n",
    "        return loss\n",
    "            \n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_gradient(self,z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "    \n",
    "    def relu(self,z):\n",
    "        return np.where(z < 0, 0, z)\n",
    "\n",
    "    def relu_gradient(self, z):\n",
    "        return np.where(z < 0, 0, 1)\n",
    "        \n",
    "    #check if possible to vectorize\n",
    "    def BCE_loss(self,X, y):\n",
    "        loss = 0\n",
    "        N = len(y)\n",
    "        eps = 1e-7\n",
    "        for i in range(N):\n",
    "            y_pred,_,_ = self.feed_forward(X[i])\n",
    "            loss_i = -(y[i]*np.log(y_pred+eps) + (1-y[i])*np.log(1-y_pred+eps))\n",
    "            loss = loss + loss_i/N\n",
    "        return loss\n",
    "    \n",
    "    def BCE_gradient(self,y,y_pred):\n",
    "        #return y_pred-y\n",
    "        eps = 1e-7\n",
    "        return (-y/(y_pred+eps) + (1-y)/(1-y_pred+eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, loss : [0.70279451]\n",
      "Iteration : 700000, loss : [0.36633528]\n",
      "Iteration : 1400000, loss : [0.36845012]\n",
      "Iteration: 1500000\n",
      "Decay, lr : 0.01\n",
      "Decay, lr : 0.002\n",
      "\n",
      "Iteration : 2100000, loss : [0.33969262]\n",
      "Iteration : 2800000, loss : [0.33148]\n",
      "Iteration: 3000000\n",
      "Decay, lr : 0.002\n",
      "Decay, lr : 0.0004\n",
      "\n",
      "Iteration : 3499999, loss : [0.31556049]\n",
      "Iteration : 0, loss : [0.6911654]\n",
      "Iteration : 700000, loss : [0.37842204]\n",
      "Iteration : 1400000, loss : [0.38899488]\n",
      "Iteration: 1500000\n",
      "Decay, lr : 0.01\n",
      "Decay, lr : 0.002\n",
      "\n",
      "Iteration : 2100000, loss : [0.36531528]\n",
      "Iteration : 2800000, loss : [0.36212953]\n",
      "Iteration: 3000000\n",
      "Decay, lr : 0.002\n",
      "Decay, lr : 0.0004\n",
      "\n",
      "Iteration : 3499999, loss : [0.35352894]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.35352894])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "in_dim = X_1.shape[1]\n",
    "n_h1 = 60\n",
    "n_h2 = 60\n",
    "n_h3 = 30\n",
    "# n_h4 = 30\n",
    "# n_h5 = 30\n",
    "# n_h6 = 100\n",
    "# n_h7 = 100\n",
    "out_dim = 1\n",
    "dimensions = [in_dim, n_h1,n_h2,n_h3,out_dim]\n",
    "activations = ['relu','relu','relu','sigmoid']\n",
    "gamma = 0.01\n",
    "weight_decay = 0.001\n",
    "mlp_1 = MLP(gamma = gamma, dimensions = dimensions, activations = activations,\n",
    "          weight_decay = weight_decay)\n",
    "mlp_2 = MLP(gamma = gamma, dimensions = dimensions, activations = activations,\n",
    "          weight_decay = weight_decay)\n",
    "mlp_1.train(X_1_train,y_1_train,max_iter = 3500000,decay_rate = 5,decay_iteration = 1500000,decay = True)\n",
    "mlp_2.train(X_2_train,y_2_train,max_iter = 3500000,decay_rate = 5,decay_iteration = 1500000,decay = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8631558857209971\n",
      "0.8422583376400603\n"
     ]
    }
   ],
   "source": [
    "#train accuracy\n",
    "# y_1_pred = mlp_1.predict(X_1_train)\n",
    "# acc1 = 1-np.sum(np.abs(y_1_pred - y_1_train)) / X_1_train.shape[0]\n",
    "# print(acc1)\n",
    "y_2_pred = mlp_2.predict(X_2_train)\n",
    "acc2 = 1-np.sum(np.abs(y_2_pred - y_2_train)) / X_2_train.shape[0]\n",
    "print(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8675034867503486\n",
      "0.8403430644895267\n"
     ]
    }
   ],
   "source": [
    "#test accuracy\n",
    "# y_1_pred = mlp_1.predict(X_1_test)\n",
    "# acc1 = 1-np.sum(np.abs(y_1_pred - y_1_test)) / X_1_test.shape[0]\n",
    "# print(acc1)\n",
    "y_2_pred = mlp_2.predict(X_2_test)\n",
    "acc2 = 1-np.sum(np.abs(y_2_pred - y_2_test)) / X_2_test.shape[0]\n",
    "print(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_,X_sub,ids = load_csv_data(\"test.csv\")\n",
    "X_sub = np.where(X_sub == -999., np.nan, X_sub)\n",
    "no_nan_idxs = ~np.isnan(X_sub).any(axis=1)\n",
    "nan_idxs = np.isnan(X_sub).any(axis=1)\n",
    "\n",
    "X_sub_1 = X_sub[~np.isnan(X_sub).any(axis=1)]\n",
    "X_sub_2 = X_sub[np.isnan(X_sub).any(axis=1)]\n",
    "\n",
    "X_sub_1 = preproc(X_sub_1)\n",
    "X_sub_2 = preproc(X_sub_2)\n",
    "\n",
    "sub_1_pred = mlp_1.predict(X_sub_1)\n",
    "sub_2_pred = mlp_2.predict(X_sub_2)\n",
    "sub_1_pred = sub_1_pred*2 -1\n",
    "sub_2_pred = sub_2_pred*2 -1\n",
    "sub_pred = np.zeros(X_sub.shape[0])\n",
    "sub_pred[no_nan_idxs] = sub_1_pred\n",
    "sub_pred[nan_idxs] = sub_2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3693839553144985"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids, sub_pred, \"nn_split_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
